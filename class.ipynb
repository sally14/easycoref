{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "import logging;\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import neuralcoref\n",
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "import colorama\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "\n",
    "class CorefModel:\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "    def import_dataset(self,path,colnames):\n",
    "        \"\"\"\n",
    "        Import the dataset of interest, check if colnames is at the right format and set dataset and colnames attributes\n",
    "        Args:\n",
    "            path: string \n",
    "                pathfile of the dataset the will be used for coreference detection\n",
    "            colnames: str or list of str if multiple columns\n",
    "                columns of the dataset for which we want to predict coreference chain\n",
    "        Returns:\n",
    "            df: dataset\n",
    "        \"\"\"   \n",
    "        if filetype==\"csv\":\n",
    "            df = pd.read_csv(path)\n",
    "        elif filetype==\"jsonl\":\n",
    "            df = pd.read_json(path, orient='records', lines=True)\n",
    "        else:\n",
    "            raise ValueError(f'Type of file {path} is not handled. Filetype must be csv or jsonl')\n",
    "        self.df = df\n",
    "        # Check if the columns are at the right format and set attribute colnames\n",
    "        if type(colnames) == list :\n",
    "            self.colnames = colnames\n",
    "        else : \n",
    "            if type(colnames)==str:\n",
    "                self.colnames = [colnames]  \n",
    "            else :\n",
    "                print('Argument colnames is not a list of string or a string')\n",
    "                raise TypeError      \n",
    "\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset before\n",
    "        Check if the columns of interest are strings and prepocess the columns\n",
    "        Returns:\n",
    "            df: dataset\n",
    "        \"\"\"   \n",
    "        for col in self.colnames :\n",
    "            # Check if columns are strings \n",
    "            if self.df.dtypes[col] == str :\n",
    "                # Replace wrong typos\n",
    "                self.df[col] = self.df[col].str.replace('\\n','. ')\n",
    "                self.df[col] = self.df[col].str.replace('  ',' ')\n",
    "            elif self.df.dtypes[col] == object :\n",
    "                self.df[col] = self.df[col].astype(str)\n",
    "                self.df[col] = self.df[col].str.replace('\\n','. ')\n",
    "                self.df[col] = self.df[col].str.replace('  ',' ')\n",
    "            else :\n",
    "                print(f\"Column type {col} is not string\")\n",
    "                raise TypeError\n",
    "            return self.df\n",
    "    \n",
    "\n",
    "    def __transform_neuralcoref__(self):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset and clean before\n",
    "        Set the dataset to an adapted form for evaluation using NeuralCoref model\n",
    "        Returns: \n",
    "            df_eval: dataset \n",
    "        \"\"\" \n",
    "        # NeuralCoref only needs dataset with the columns of interest\n",
    "        self.df_eval = self.df\n",
    "        return self.df_eval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __split_into_sentences__(self, text):\n",
    "        \"\"\" \n",
    "        -- Sub-function : step of __transform_e2ecoref__ --\n",
    "        Split a text into a list of sentences\n",
    "        Args: \n",
    "            text : str\n",
    "        Returns:\n",
    "            sentences : list of str \n",
    "                list of the text sentences\n",
    "        \"\"\"\n",
    "\n",
    "        # Typos needed\n",
    "        alphabets= \"([A-Za-z])\"\n",
    "        prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "        suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "        starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "        acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "        websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "        # Fixe the typos\n",
    "        text = \" \" + text + \"  \"\n",
    "        text = text.replace(\"\\n\",\" \")\n",
    "        text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "        text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "        if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "        text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "        text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "        text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "        text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "        text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "        text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "        text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "        if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "        if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "        if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "        if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "        text = text.replace(\".\",\".<stop>\")\n",
    "        text = text.replace(\"?\",\"?<stop>\")\n",
    "        text = text.replace(\"!\",\"!<stop>\")\n",
    "        text = text.replace(\"<prd>\",\".\")\n",
    "\n",
    "        # Split into sentences\n",
    "        sentences = text.split(\"<stop>\")\n",
    "        sentences = sentences[:-1]\n",
    "        sentences = [s.strip() for s in sentences]\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def __formatage_liste__(self, text):\n",
    "        \"\"\" \n",
    "        -- Sub-function : step of __transform_e2ecoref__ --\n",
    "        Split the text into a list of list of words for each sentence and associate it a list of list of speakers for each word\n",
    "        \n",
    "        Args: \n",
    "            text : str\n",
    "        Returns:\n",
    "            [liste_formate, liste_speaker] : list of two lists\n",
    "                liste_formate: list of lists \n",
    "                    list of words for each sentence of a text\n",
    "                liste_speaker: list of lists \n",
    "                    speaker for each word of liste_formate (empty strings by default)\n",
    "        \"\"\"\n",
    "        liste_formate = []\n",
    "        liste_speaker = []\n",
    "\n",
    "        # Create a list of sentences\n",
    "        liste_sentence = self.__split_into_sentences__(text)\n",
    "\n",
    "        # Transform each sentence to a list of words and create a list of empty strings\n",
    "        for sentence in liste_sentence :\n",
    "            liste_mot = list(sentence.split(\" \")) \n",
    "            liste_speak = [\"\" for i in liste_mot]\n",
    "\n",
    "            # Add them to the \"global\" list\n",
    "            liste_formate.append(liste_mot)\n",
    "            liste_speaker.append(liste_speak)\n",
    "\n",
    "        return [liste_formate, liste_speaker]\n",
    "\n",
    "\n",
    "    def __dico__(self, text):\n",
    "        \"\"\" \n",
    "        -- Sub-function : step of __transform_e2ecoref__ --\n",
    "        Create a dictionnary for a text under a specific format, needed to use e2ecoref\n",
    "\n",
    "        Args: \n",
    "            text : str\n",
    "        Returns:\n",
    "            dico : dictionnary\n",
    "                dictionnary for the text under the following format :\n",
    "                    - clusters : where the coreference chains of the text will be added\n",
    "                    - doc_key : genre of the text, set by default to \"nw\" news wire, but can be changed (\"bc\": broadcast conversations, \n",
    "                    \"bn\": broadcast news, \"mz\": magazines, \"nw\": news wire, \"pt\": pivot corpus, \"tc\": telephone conversation, \"wb\": weblogs)\n",
    "                    - sentences : list of the text sentences, each being under a list of words format\n",
    "                    - speakers : speaker for each word, respecting the same format as previous \"sentences\" \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        dico={\n",
    "        \"clusters\": [],\n",
    "        \"doc_key\": \"nw\",\n",
    "        \"sentences\": self.__formatage_liste__(text)[0],\n",
    "        \"speakers\": self.__formatage_liste__(text)[1]\n",
    "        }\n",
    "\n",
    "        return dico\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __transform_e2ecoref__(self,col):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset and clean before\n",
    "        For one specific column, create a json file to an adapted form for inference with e2eCoref model. Can be used successively \n",
    "        if there are several column of interest for one dataset.\n",
    "        Args:\n",
    "            col : str\n",
    "                name of the specific column for which we want to use the model\n",
    "        Returns: \n",
    "            df_eval : dataset \n",
    "                dataset of the json file with the following columns :\n",
    "                    - clusters : where the coreference chains of the text will be added\n",
    "                    - doc_key : genre of the text, set by default to \"nw\" news wire, but can be changed (\"bc\": broadcast conversations, \n",
    "                    \"bn\": broadcast news, \"mz\": magazines, \"nw\": news wire, \"pt\": pivot corpus, \"tc\": telephone conversation, \"wb\": weblogs)\n",
    "                    - sentences : list of the text sentences, each being under a list of words format\n",
    "                    - speakers : speaker for each word, respecting the same format as previous \"sentences\" \n",
    "\n",
    "        \"\"\"  \n",
    "\n",
    "        # Create the dictionnary  \n",
    "        dicos_list = []\n",
    "\n",
    "        # For each text of the column create a dico under the right format\n",
    "        for text in self.df[col]:\n",
    "            dicos_list.append(self.__dico__(text))\n",
    "\n",
    "        # Saving the dataset under a json temporary file \n",
    "        self.fp1 = tempfile.NamedTemporaryFile(mode='ab')\n",
    "        datapath = self.fp1.name\n",
    "  \n",
    "        # For each line of the dictionnary\n",
    "        for dico in dicos_list :\n",
    "            self.fp1.write(bytes(json.dumps(dico), 'utf-8'))\n",
    "            self.fp1.write(b'\\n')\n",
    "        \n",
    "        # Read the saved json temporary file \n",
    "        df_eval = pd.read_json(datapath, orient='records', lines=True, encoding='utf-8')\n",
    "        \n",
    "        return df_eval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __neuralcoref__(self,col):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset, clean and __transform_neuralcoref__ before\n",
    "        Gives the coreference chain clusters of each text for column col of the dataset, using the NeuralCoref model \n",
    "\n",
    "        Args:\n",
    "            col : str\n",
    "                name of the specific column for which we want to use the model\n",
    "        Returns: \n",
    "            column_coref : list of list of lists of spans\n",
    "                future dataset column, each element i being a list of every coreference clusters \n",
    "                found by the model for text line i of the dataset. A coreference cluster is a list of \n",
    "                text \"spans\" (specific class used by NeuralCoref model)\n",
    "        \"\"\" \n",
    "\n",
    "        column_coref = []\n",
    "        # For each text of the dataset\n",
    "        for i in range(len(self.df_eval)):\n",
    "            text = self.df_eval[col][i]\n",
    "            text_nlp = nlp(text)\n",
    "            # Use neuralcoref module to give the coreference chains clusters of the text\n",
    "            column_coref.append(text_nlp._.coref_clusters)\n",
    "\n",
    "        return column_coref\n",
    "            \n",
    " \n",
    "    def __e2ecoref__(self,col):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset, clean and __transform_e2ecoref__ before\n",
    "        Gives the dataframe presenting results of coreference detection using e2eCoref model for one specific column of text.\n",
    "        Args: \n",
    "            col : str \n",
    "                name of the specific column for which we want to use the model\n",
    "        Returns: \n",
    "            df_coref[col] : dataframe \n",
    "                dataframe with the following columns :\n",
    "                    (each column line corresponding to the evaluation of the text of the dataframe line i)\n",
    "                    - clusters : empty lists\n",
    "                    - doc key : \"nw\" by default (news wire)\n",
    "                    - sentences : list of lists, each sentences of the text splitted into list of words\n",
    "                    - speakers : list of lists, gives speakers for each word, following previous \"sentences\" format\n",
    "                    - predicted_clusters : list of lists, for each clusters of coreference chain found gives list of mention positions \n",
    "        \"\"\"\n",
    "\n",
    "        # Call the temporary file used for method __transform_e2ecoref__ (which create the file used for evaluation)\n",
    "        datapath = self.fp1.name \n",
    "        # Create new temporary file for evaluation output\n",
    "        self.fp2 = tempfile.NamedTemporaryFile(mode='ab')\n",
    "        output = self.fp2.name\n",
    "\n",
    "        # Prediction using e2eCoref\n",
    "        os.system(f'python predict.py final {datapath} {output}')\n",
    "        self.fp1.close()\n",
    "\n",
    "        df_coref = pd.read_json(output, orient='records', lines=True, encoding='utf-8')\n",
    "        setattr(self, f'df_coref_{col}', df_coref)\n",
    "        self.fp2.close()\n",
    "\n",
    "        return getattr(self, f'df_coref_{col}')\n",
    "\n",
    "\n",
    "    def inference(self,model):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset and clean before\n",
    "        Arguments : self, model (NeuralCoref or e2eCoref)\n",
    "        Detect and extract coreference chains for each text and column of the dataframe and present the results in a new dataframe.\n",
    "        \n",
    "        If model is e2ecoref, also create a dataframe used later for standardized results :\n",
    "        df_useful : dataframe\n",
    "             for each column col : \n",
    "                - text_list_col : text under list format\n",
    "                - predicted_clusters_col : coreference mentions positions under list format\n",
    "        \n",
    "        Args: \n",
    "            model : str \n",
    "                name of the model we want to use for coreference detection (neuralcoref or e2ecoref)\n",
    "        Returns: \n",
    "            df_results : dataframe \n",
    "\n",
    "            if model is neuralcoref :\n",
    "                dataframe with the following columns, for each column col :\n",
    "                    (each column line corresponding to the evaluation of the text of the dataframe line i)\n",
    "                    - col : texts of the original dataframe\n",
    "                    - clusters_col : list of lists of spans, for each clusters of coreference chain gives list of spans\n",
    "                    (this specific class used by NeuralCoref contains in itself the span start and end positions) \n",
    "\n",
    "            if model is e2coref :\n",
    "                dataframe with the following columns, for each column col :\n",
    "                    (each column line corresponding to the evaluation of the text of the dataframe line i)\n",
    "                    - col : texts of the original dataframe\n",
    "                    - clusters_col : list of lists of strings, for each clusters of coreference chain gives list of mentions\n",
    "        \"\"\"\n",
    "\n",
    "        # Create dataframe for results \n",
    "        self.df_results = pd.DataFrame()\n",
    "\n",
    "        # According to the model given in argument, create columns of results\n",
    "\n",
    "        if model == \"neuralcoref\":\n",
    "            # Run the method required before using the methode __neuralcoref__\n",
    "            self.df_eval = self.__transform_neuralcoref__()\n",
    "\n",
    "            for col in self.colnames :\n",
    "                # Column of col texts\n",
    "                self.df_results[col] = self.df[col]\n",
    "                # Column of clusters found for each col\n",
    "                self.df_results[f'clusters_{col}'] = self.__neuralcoref__(col)\n",
    "\n",
    "        elif model == \"e2ecoref\" :\n",
    "            # Create dataframe for further use \n",
    "            self.df_useful = pd.DataFrame()\n",
    "\n",
    "            # For each column of colnames\n",
    "            for col in self.colnames :\n",
    "                # Run methods required \n",
    "                df_eval = self.__transform_e2ecoref__(col)\n",
    "                df_coref = self.__e2ecoref__(col)\n",
    "\n",
    "                # Column of col texts\n",
    "                self.df_results[col] = self.df[col]\n",
    "            \n",
    "                # Give the mention string of the mentions positions given by column predicted_clusters\n",
    "                column_cluster = []\n",
    "                column_text_list = []\n",
    "\n",
    "                for i in range(len(self.df_results)):\n",
    "                    liste_clusters = []\n",
    "                    list_clusters_num = df_coref[\"predicted_clusters\"][i]\n",
    "                    list_sentences = df_coref['sentences'][i]\n",
    "                    # Text under list format\n",
    "                    flat_list_sentences = [item for sublist in list_sentences for item in sublist]\n",
    "\n",
    "                    for cluster in list_clusters_num :\n",
    "                        # List of the cluster strings\n",
    "                        cluster_str = []\n",
    "                        # For each mention of the coreference chain\n",
    "                        for item in cluster :\n",
    "                            mention_start = item[0]\n",
    "                            mention_end = item[1] + 1\n",
    "                            # Add the mention string to the cluster of strings\n",
    "                            cluster_str.append(flat_list_sentences[mention_start:mention_end])\n",
    "\n",
    "                        # Add the cluster of strings to the list of clusters\n",
    "                        liste_clusters.append(cluster_str)\n",
    "\n",
    "                    # Add the list of clusters of text line i to the column\n",
    "                    column_cluster.append(liste_clusters)\n",
    "                    # Add the text under list format to the column\n",
    "                    column_text_list.append(flat_list_sentences)\n",
    "\n",
    "                # Column of col texts\n",
    "                self.df_results[f'clusters_{col}'] = column_cluster\n",
    "\n",
    "                # Column of text_list add to useful dataframe\n",
    "                self.df_useful[f'text_list_{col}'] = column_text_list\n",
    "                # Column of predicted_clusters (mentions positions under list format) add to useful dataframe\n",
    "                self.df_useful[f'predicted_clusters_{col}'] = df_coref[\"predicted_clusters\"]\n",
    "                \n",
    "        else:\n",
    "            print('This model is not manageable with CorefModel')\n",
    "            raise NameError\n",
    "        \n",
    "        return self.df_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __isprefixe__(self,i,mot,texte): \n",
    "        \"\"\" \n",
    "         -- Sub-function : step of standardization and visualisation --\n",
    "        Check if a word has an occurrence in a text in position i\n",
    "        Args: \n",
    "            i : int\n",
    "            mot : str\n",
    "            texte : str\n",
    "        Returns: \n",
    "            B : bool\n",
    "        \"\"\"\n",
    "\n",
    "        B = True\n",
    "        j=0\n",
    "        while (j < len(mot)) and B:\n",
    "            if texte[i+j] != mot[j]:\n",
    "                B = False\n",
    "            j+= 1 \n",
    "        return B\n",
    "\n",
    "\n",
    "    def __positions_str__(self,mention_str,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give list of occurring positions of a mention in a text\n",
    "        Args: \n",
    "            mention_str : str\n",
    "            texte : str \n",
    "        Returns: \n",
    "            occ : list \n",
    "                list of the occurring positions of the mention in the text\n",
    "        \"\"\"\n",
    "        occ = []\n",
    "        for i in range(len(texte)-len(mention_str)+1):\n",
    "            if self.__isprefixe__(i,mention_str,texte): \n",
    "                occ.append(i)\n",
    "\n",
    "        return occ\n",
    "\n",
    "\n",
    "    def __positions_span__(self, mention_str,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give list of occurring start span positions of a mention in a text\n",
    "        Args:\n",
    "            mention_str : str\n",
    "            texte : str\n",
    "        Returns:\n",
    "            occ1 : list \n",
    "                list of the start span positions of the mention in the text\n",
    "        \"\"\"\n",
    "        occ1 = []\n",
    "        for i in self.__positions_str__(mention_str,texte): \n",
    "            \n",
    "            chaine = texte[0:i+len(mention_str)]\n",
    "            mention_span = nlp(mention_str)\n",
    "            chain = nlp(chaine)\n",
    "\n",
    "            occ1.append(len(chain)-len(mention_span))\n",
    "    \n",
    "        return occ1\n",
    "\n",
    "    def __positions_list__(self,mention,texte):\n",
    "        \"\"\"\n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give list of start and end positions of a mention in a text\n",
    "        Args:\n",
    "            mention : str \n",
    "            texte : str\n",
    "        Returns: \n",
    "            occ2 : list of lists ([start,end] format)\n",
    "                list of the start and end list positions of the mention in the text\n",
    "        \"\"\"\n",
    "\n",
    "        occ2 = []\n",
    "        for i in self.__positions_str__(mention,texte): \n",
    "            \n",
    "            # To handle text part without any complete sentence\n",
    "            chaine = texte[:i] +  \".\"\n",
    "\n",
    "            liste = self.__formatage_liste__(chaine)[0]\n",
    "\n",
    "            # Transform list of lists to a simple list\n",
    "            liste_flat = [item for sublist in liste for item in sublist]\n",
    "\n",
    "            # Handle empty list\n",
    "            if liste_flat == [] :\n",
    "                liste_flat = chaine.split(\" \")\n",
    "            liste_flat.pop()\n",
    "           \n",
    "            mention_list = mention.split(\" \")\n",
    "            position = len(liste_flat) \n",
    "            occ2.append([position,position+len(mention_list)])\n",
    " \n",
    "        return occ2\n",
    "\n",
    "\n",
    "    def __position_span_to_str__(self,mention,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give the start string position of a mention in a text based on its span positions\n",
    "        Args: \n",
    "            mention : span\n",
    "            texte : str \n",
    "        Returns: \n",
    "            position_finale : int\n",
    "                start string position of the mention in the text\n",
    "        \"\"\"\n",
    "\n",
    "        mention_str = mention.text\n",
    "\n",
    "        span_position = mention.start \n",
    "\n",
    "        # Function returning the list of string positions of the mention in the text\n",
    "        liste_pos_str = self.__positions_str__(mention_str,texte) \n",
    "        # Function returning the list of span positions of the mention in the text\n",
    "        liste_pos_span = self.__positions_span__(mention_str,texte) \n",
    "        \n",
    "        # Check if the span position of the mention is in the list of span positions of the mention in the text\n",
    "        if span_position in liste_pos_span :\n",
    "            # Take the list index of that span positions\n",
    "            ind = liste_pos_span.index(span_position)\n",
    "            # Take the parallel string position corresponding to that index\n",
    "            position_finale = liste_pos_str[ind]\n",
    "    \n",
    "        return position_finale \n",
    "\n",
    "    def __position_str_to_span__(self,start,end,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give start and end span positions of a mention in a text based on its string positions\n",
    "        Args: \n",
    "            start : int\n",
    "                start str position of a mention\n",
    "            end : int\n",
    "                end str position of a mention\n",
    "            texte : str \n",
    "        Returns: \n",
    "            list ([start, end] format)\n",
    "                start and end span positions of the mention in the text\n",
    "        \"\"\"\n",
    "        mention_str = texte[start:end]\n",
    "        mention_span = nlp(mention_str)\n",
    "\n",
    "        chaine = texte[0:end]\n",
    "        chain = nlp(chaine)\n",
    "\n",
    "        return ([len(chain)-len(mention_span),len(chain)])\n",
    "\n",
    "    def __position_list_to_str__(self,position,mention,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give start str position of a mention in a text based on its start list position\n",
    "        Args: \n",
    "            position : int\n",
    "                start position of the mention under list format\n",
    "            mention : str\n",
    "            texte : str\n",
    "        Returns: \n",
    "            position_finale : int\n",
    "                start string position of the mention in the text\n",
    "        \"\"\"\n",
    "        # Function returning the list of string positions of the mention in the text\n",
    "        liste_pos_str = self.__positions_str__(mention,texte)\n",
    "        # Function returning the list of list positions of the mention in the text\n",
    "        liste_pos_list = self.__positions_list__(mention,texte)\n",
    "\n",
    "        # Check if the list position of the mention is in the list of list positions of the mention in the text\n",
    "        if position in liste_pos_list :\n",
    "            # Take the list index of that list positions\n",
    "            ind = liste_pos_list.index(position)\n",
    "            # Take the parallel string position corresponding to that index\n",
    "            position_finale = liste_pos_str[ind]\n",
    "      \n",
    "        return position_finale \n",
    "\n",
    "\n",
    "\n",
    "    def __no_doublons__(self,clusters):\n",
    "        \"\"\" \n",
    "        -- Sub-function : step of visualisation --\n",
    "        Find every overlapping mentions of detected coreference chains for a text, \n",
    "        and only keep the one with the best coreference score\n",
    "        Args: \n",
    "            clusters : list of lists of spans\n",
    "                all clusters of coreference chain found by neuralcoref for a particular text\n",
    "        Returns: list\n",
    "                list of mentions to supress because they overlaps others \n",
    "        \"\"\"\n",
    "        liste_positions = []\n",
    "        liste_mentions = []\n",
    "        liste_mentions_a_suppr = []\n",
    "        for clust in clusters :\n",
    "            cluster = clust.mentions\n",
    "            \n",
    "        \n",
    "            for mention in cluster:\n",
    "                # List of all intervall of spans\n",
    "                liste_positions.append(pd.Interval(mention.start, mention.end))\n",
    "                # List of all spans\n",
    "                liste_mentions.append(mention) \n",
    "\n",
    " \n",
    "        # Observe if some overlaps each others\n",
    "        for interval1 in liste_positions :\n",
    "            for interval2 in liste_positions :\n",
    "                if interval1.overlaps(interval2) and interval1 != interval2 :\n",
    "\n",
    "                    i1 = liste_positions.index(interval1) \n",
    "                    i2 = liste_positions.index(interval2)\n",
    "                    mention1 = liste_mentions[i1]\n",
    "                    mention2 = liste_mentions[i2]\n",
    "\n",
    "\n",
    "                    dico1 = mention1._.coref_scores\n",
    "                    score1 = max(dico1.values())\n",
    "\n",
    "                    dico2 = mention2._.coref_scores\n",
    "                    score2 = max(dico2.values())\n",
    "                \n",
    "                    # Add the mention with the lower score to the list of mention to suppress\n",
    "                    if score1 <= score2 and [mention1.start,mention1.end] not in liste_mentions_a_suppr :\n",
    "                        liste_mentions_a_suppr.append([mention1.start, mention1.end])\n",
    "                    \n",
    "                    elif score1 > score2 and [mention2.start,mention2.end] not in liste_mentions_a_suppr :\n",
    "                        liste_mentions_a_suppr.append([mention2.start, mention2.end])\n",
    "                    \n",
    "\n",
    "        return(liste_mentions_a_suppr)\n",
    "\n",
    "\n",
    "\n",
    "    def __standardized_results__(self,model):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset, clean and inference before\n",
    "        Gives a dataframe of standardized results that will be useful for visualization\n",
    "        Args: \n",
    "            model : str \n",
    "                name of the model we want to use for coreference detection (neuralcoref or e2ecoref)     \n",
    "        Returns: \n",
    "            df_standardized : dataframe \n",
    "                dataframe with the following columns, for each column col  :\n",
    "                    (each column line corresponding to the evaluation of the text of the dataframe line i)\n",
    "                    - col : texts of the original dataframe \n",
    "                    - clusters_col : list of lists, for each clusters of coreference chain gives list of mentions \n",
    "                    (if model is neuralcoref mentions are spans and if model is e2ecoref mentions are strings) \n",
    "                    - span_positions_col : list of list of lists (format [index_start,index_end]), for each clusters of coreference \n",
    "                    chain gives list of mention position under span format\n",
    "        \"\"\"\n",
    "\n",
    "        if model == \"neuralcoref\":\n",
    "            # Dataframe with columns col and clusters_col\n",
    "            self.df_standardized = self.df_results\n",
    "\n",
    "            # Build columns span_positions_col \n",
    "            for col in self.colnames :\n",
    "\n",
    "                # Create column giving the span positions of the mentions of coreference chains of each text\n",
    "                column_span_pos = []\n",
    "                for i in range(len(self.df_standardized)) :\n",
    "                    # List of lists : span positions of every mention of every cluster for one text\n",
    "                    text_span_pos = []\n",
    "\n",
    "                    # Mentions to suppress\n",
    "                    mentions_a_supp = self.__no_doublons__(self.df_standardized[f'clusters_{col}'][i])\n",
    "                    \n",
    "\n",
    "                    for clusters in self.df_standardized[f'clusters_{col}'][i]:\n",
    "                        cluster = clusters.mentions\n",
    "                        cluster = [mention for mention in cluster if [mention.start,mention.end] not in mentions_a_supp]\n",
    "                        \n",
    "                \n",
    "                        # List of span positions of every mention of one cluster\n",
    "                        cluster_span_pos = []\n",
    "                        for mention in cluster :\n",
    "                            # Mention are spans : add start and end span position to the cluster \n",
    "                            cluster_span_pos.append([mention.start, mention.end])\n",
    "\n",
    "                        # Add the cluster list of mention positions for each cluster \n",
    "                        text_span_pos.append(cluster_span_pos)\n",
    "                    \n",
    "                    # Add the list of lists for each text to the column\n",
    "                    column_span_pos.append(text_span_pos)\n",
    "\n",
    "                self.df_standardized[f'span_positions_{col}'] = column_span_pos\n",
    "\n",
    "\n",
    "        elif model == \"e2ecoref\" :\n",
    "            # Dataframe with columns col and clusters_col\n",
    "            self.df_standardized = self.df_results\n",
    "\n",
    "            # Build columns span_positions_col \n",
    "            for col in self.colnames:\n",
    "\n",
    "                # Create column giving the string positions of the mentions of coreference chains of each text\n",
    "                column_str_pos = []\n",
    "\n",
    "                for i in range(len(self.df_useful)):\n",
    "                    # List of lists : string positions of every mention of every cluster for one text\n",
    "                    text_str_pos = []\n",
    "\n",
    "                    for cluster in self.df_useful[f'predicted_clusters_{col}'][i] :\n",
    "                        # List of string positions of every mention of one cluster\n",
    "                        cluster_str_pos = []\n",
    "\n",
    "                        # Convert list positions to string positions\n",
    "                        for positions in cluster:\n",
    "                            start = positions[0]\n",
    "                            end = positions[1]+1\n",
    "                            positions_corr = [start,end]\n",
    "\n",
    "                            mention = self.df_useful[f'text_list_{col}'][i][start:end]\n",
    "                            mention_str = \" \".join(mention)\n",
    "                    \n",
    "                            texte = self.df_standardized[col][i]\n",
    "                            \n",
    "                            # Start string position of the mention\n",
    "                            pos_fin = self.__position_list_to_str__(positions_corr,mention_str,texte)\n",
    "\n",
    "                            # Add start and end string positions to the list \n",
    "                            cluster_str_pos.append([pos_fin,pos_fin+len(mention_str)])\n",
    "\n",
    "                        # Add the cluster list of mention positions for each cluster \n",
    "                        text_str_pos.append(cluster_str_pos)\n",
    "\n",
    "                    # Add the list of lists for each text to the column\n",
    "                    column_str_pos.append(text_str_pos)\n",
    "\n",
    "                self.df_useful[f'str_pos_{col}'] = column_str_pos\n",
    "\n",
    "                # Thanks to that new column, create column giving the spans positions of the mentions of coreference chains of each text\n",
    "                column_span_pos = []\n",
    "                for i in range(len(self.df_useful)):\n",
    "                    text_span_pos = []\n",
    "                    for cluster in self.df_useful[f'str_pos_{col}'][i] :\n",
    "                        cluster_span_pos = []\n",
    "                        for pos_str in cluster :\n",
    "                            start = pos_str[0]\n",
    "                            end = pos_str[1]\n",
    "                            texte = self.df_standardized[col][i]\n",
    "                            pos = self.__position_str_to_span__(start,end,texte)\n",
    "                    \n",
    "                            cluster_span_pos.append(pos)\n",
    "\n",
    "                        # Add the cluster list of mention positions for each cluster \n",
    "                        text_span_pos.append(cluster_span_pos)\n",
    "\n",
    "                    # Add the list of lists for each text to the column  \n",
    "                    column_span_pos.append(text_span_pos)\n",
    "\n",
    "                self.df_standardized[f'span_positions_{col}']= column_span_pos\n",
    "            \n",
    "        else:\n",
    "            print('This model is not manageable with CorefModel')\n",
    "            raise NameError\n",
    "\n",
    "        \n",
    "        return self.df_standardized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def visualisation(self,model,col,i):\n",
    "        \"\"\" \n",
    "        Requires to have run the method import_dataset, clean and inference before. \n",
    "        Given a text, highlights all the coreference chains detected by the chosen model. \n",
    "        This function must be printed to see the highlights in different colors.\n",
    "        Args: \n",
    "            model : str\n",
    "                name of the model we want to use for coreference detection (neuralcoref or e2ecoref)\n",
    "            col : str\n",
    "                column of interest (in colnames)\n",
    "            i : int\n",
    "                line of interest (in the column)\n",
    "        Returns:  \n",
    "            texte : str\n",
    "                text with all of it coreference chains underlined in different colors\n",
    "        \"\"\"\n",
    "        if model not in ['neuralcoref', 'e2ecoref']:\n",
    "            print('This model is not manageable with CorefModel')\n",
    "            raise NameError\n",
    "        \n",
    "        else:\n",
    "            self.df_standardized = self.__standardized_results__(model)\n",
    "            texte = self.df_standardized[col][i]\n",
    "            texte_or = texte \n",
    "            nlp_texte = nlp(texte)\n",
    "                \n",
    "            liste_charactere = [i for i in range(len(texte))]\n",
    "            liste_charactere_updated = [i for i in range(len(texte))]\n",
    "\n",
    "            # Font color\n",
    "            color = 0 \n",
    "            colors = 240 \n",
    "\n",
    "            clusters_positions = self.df_standardized[f'span_positions_{col}'][i]\n",
    "            \n",
    "            for cluster in clusters_positions :\n",
    "                color += 1\n",
    "                \n",
    "                if len(cluster)>1 :\n",
    "                    for positions in cluster :\n",
    "\n",
    "                        # Positions in spans\n",
    "                        mention_start = positions[0]\n",
    "                        mention_end = positions[1]\n",
    "\n",
    "                        # Mention in span\n",
    "                        mention = nlp_texte[mention_start:mention_end]\n",
    "                        # Mention in str\n",
    "                        mention_str = (nlp_texte[mention_start:mention_end]).text \n",
    "\n",
    "                        # Mention start position in strings\n",
    "                        index_position_start = self.__position_span_to_str__(mention,texte_or) \n",
    "                        position_start = liste_charactere_updated[index_position_start]\n",
    "                        # Mention end position in strings\n",
    "                        position_end = position_start+len(mention_str) \n",
    "\n",
    "                        # Text from beginning to mention\n",
    "                        deb = texte[0: position_start] \n",
    "                        # End of the text\n",
    "                        fin = texte[position_end:] \n",
    "\n",
    "                        # Rewrite the text\n",
    "                        texte = deb + f'\\033[38;5;{color}m' + f'\\x1b[48;5;{colors}m' + mention_str + '\\033[0;0m' + fin #on modifie texte en changeant la couleur de la mention\n",
    "                        add1 = len(f'\\033[38;5;{color}m') + len(f'\\x1b[48;5;{colors}m')\n",
    "                        add2 = len('\\033[0;0m')\n",
    "\n",
    "\n",
    "                        # Update positions of text element after adding add1\n",
    "                        for i in range(index_position_start,len(liste_charactere_updated)): \n",
    "                            liste_charactere_updated[i] += add1\n",
    "                        # Update positions of text element after adding add2\n",
    "                        for i in range(index_position_start+len(mention_str),len(liste_charactere_updated)): \n",
    "                            liste_charactere_updated[i] += add2\n",
    "                \n",
    "                return texte\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==2.1.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (1.19.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (0.8.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (2.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (1.0.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (0.9.6)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (7.0.8)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (0.2.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (4.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.62.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: neuralcoref in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from neuralcoref) (1.19.5)\n",
      "Requirement already satisfied: boto3 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from neuralcoref) (1.18.32)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from neuralcoref) (2.25.1)\n",
      "Requirement already satisfied: spacy>=2.1.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from neuralcoref) (2.1.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.26.6)\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (0.8.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.62.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from boto3->neuralcoref) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from boto3->neuralcoref) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.32 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from boto3->neuralcoref) (1.21.32)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from botocore<1.22.0,>=1.21.32->boto3->neuralcoref) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.32->boto3->neuralcoref) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 15.2 MB/s \n",
      "\u001b[?25h\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: colorama in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (0.4.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==2.1.0\n",
    "!pip install neuralcoref --no-binary neuralcoref\n",
    "!python -m spacy download en\n",
    "!pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>title</th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>text_comp</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>It’s High Time We Fought This Virus the Americ...</td>\n",
       "      <td>The administration has all the authority it ne...</td>\n",
       "      <td>Governors and health officials tell us that th...</td>\n",
       "      <td>3 avril 2020 ET 11:00</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>It’s High Time We Fought This Virus the Americ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Carlota’s World: What Children Can Teach Us Du...</td>\n",
       "      <td>No talking face to face, no hugging or kissing...</td>\n",
       "      <td>Everything seems normal. And yet nothing is. I...</td>\n",
       "      <td>3 avril 2020 ET 17:00</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>Carlota’s World: What Children Can Teach Us Du...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Does Covid -19 Hit Women and Men Differently? ...</td>\n",
       "      <td>Data from other countries shows that more men ...</td>\n",
       "      <td>— Caroline Criado Perez, author of “Invisible ...</td>\n",
       "      <td>3 avril 2020 ET 15:42</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>Does Covid -19 Hit Women and Men Differently? ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>What We Pretend to Know About the Coronavirus ...</td>\n",
       "      <td>Today’s propaganda is tomorrow’s truth, and vi...</td>\n",
       "      <td>But we seem to be living in a nightmare scenar...</td>\n",
       "      <td>3 avril 2020 ET 17:26</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>What We Pretend to Know About the Coronavirus ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Covid -19: A Look Back From 2025</td>\n",
       "      <td>In which the coronavirus has changed almost ev...</td>\n",
       "      <td>When Covid -19 first emerged as a health crisi...</td>\n",
       "      <td>4 avril 2020 ET 01:57</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>Covid -19: A Look Back From 2025\\nIn which the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Can an Old Vaccine Stop the New Coronavirus ?</td>\n",
       "      <td>A vaccine that was developed a hundred years a...</td>\n",
       "      <td>The vaccine seems to “train” the immune system...</td>\n",
       "      <td>4 avril 2020 ET 00:09</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>Can an Old Vaccine Stop the New Coronavirus ?\\...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Calling Dr. Fauci</td>\n",
       "      <td>Our interview with the nation’s top infectious...</td>\n",
       "      <td>Negotiations to schedule a time with him were ...</td>\n",
       "      <td>4 avril 2020 ET 01:11</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>Calling Dr. Fauci\\nOur interview with the nati...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Bad News Wrapped in Protein: Inside the Corona...</td>\n",
       "      <td>A virus is “simply a piece of bad news wrapped...</td>\n",
       "      <td>Research on other coronaviruses has given scie...</td>\n",
       "      <td>3 avril 2020 ET 11:01</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>Bad News Wrapped in Protein: Inside the Corona...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>William Frankland, Pioneering Allergist, Dies ...</td>\n",
       "      <td>One of the top allergists of the 20th century,...</td>\n",
       "      <td>Dr. William Frankland, one of the top allergis...</td>\n",
       "      <td>3 avril 2020 ET 16:06</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>William Frankland, Pioneering Allergist, Dies ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>In the American South, a Perfect Storm Is Gath...</td>\n",
       "      <td>In states with many uninsured citizens, few ho...</td>\n",
       "      <td>But it had to be done. Nashville’s courts and ...</td>\n",
       "      <td>4 avril 2020 ET 01:15</td>\n",
       "      <td>The New York Times Company</td>\n",
       "      <td>In the American South, a Perfect Storm Is Gath...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "5           5             5   \n",
       "6           6             6   \n",
       "7           7             7   \n",
       "8           8             8   \n",
       "9           9             9   \n",
       "\n",
       "                                               title  \\\n",
       "0  It’s High Time We Fought This Virus the Americ...   \n",
       "1  Carlota’s World: What Children Can Teach Us Du...   \n",
       "2  Does Covid -19 Hit Women and Men Differently? ...   \n",
       "3  What We Pretend to Know About the Coronavirus ...   \n",
       "4                   Covid -19: A Look Back From 2025   \n",
       "5      Can an Old Vaccine Stop the New Coronavirus ?   \n",
       "6                                  Calling Dr. Fauci   \n",
       "7  Bad News Wrapped in Protein: Inside the Corona...   \n",
       "8  William Frankland, Pioneering Allergist, Dies ...   \n",
       "9  In the American South, a Perfect Storm Is Gath...   \n",
       "\n",
       "                                      lead_paragraph  \\\n",
       "0  The administration has all the authority it ne...   \n",
       "1  No talking face to face, no hugging or kissing...   \n",
       "2  Data from other countries shows that more men ...   \n",
       "3  Today’s propaganda is tomorrow’s truth, and vi...   \n",
       "4  In which the coronavirus has changed almost ev...   \n",
       "5  A vaccine that was developed a hundred years a...   \n",
       "6  Our interview with the nation’s top infectious...   \n",
       "7  A virus is “simply a piece of bad news wrapped...   \n",
       "8  One of the top allergists of the 20th century,...   \n",
       "9  In states with many uninsured citizens, few ho...   \n",
       "\n",
       "                                                text                   date  \\\n",
       "0  Governors and health officials tell us that th...  3 avril 2020 ET 11:00   \n",
       "1  Everything seems normal. And yet nothing is. I...  3 avril 2020 ET 17:00   \n",
       "2  — Caroline Criado Perez, author of “Invisible ...  3 avril 2020 ET 15:42   \n",
       "3  But we seem to be living in a nightmare scenar...  3 avril 2020 ET 17:26   \n",
       "4  When Covid -19 first emerged as a health crisi...  4 avril 2020 ET 01:57   \n",
       "5  The vaccine seems to “train” the immune system...  4 avril 2020 ET 00:09   \n",
       "6  Negotiations to schedule a time with him were ...  4 avril 2020 ET 01:11   \n",
       "7  Research on other coronaviruses has given scie...  3 avril 2020 ET 11:01   \n",
       "8  Dr. William Frankland, one of the top allergis...  3 avril 2020 ET 16:06   \n",
       "9  But it had to be done. Nashville’s courts and ...  4 avril 2020 ET 01:15   \n",
       "\n",
       "                       source  \\\n",
       "0  The New York Times Company   \n",
       "1  The New York Times Company   \n",
       "2  The New York Times Company   \n",
       "3  The New York Times Company   \n",
       "4  The New York Times Company   \n",
       "5  The New York Times Company   \n",
       "6  The New York Times Company   \n",
       "7  The New York Times Company   \n",
       "8  The New York Times Company   \n",
       "9  The New York Times Company   \n",
       "\n",
       "                                           text_comp  id  \n",
       "0  It’s High Time We Fought This Virus the Americ...   0  \n",
       "1  Carlota’s World: What Children Can Teach Us Du...   1  \n",
       "2  Does Covid -19 Hit Women and Men Differently? ...   2  \n",
       "3  What We Pretend to Know About the Coronavirus ...   3  \n",
       "4  Covid -19: A Look Back From 2025\\nIn which the...   4  \n",
       "5  Can an Old Vaccine Stop the New Coronavirus ?\\...   5  \n",
       "6  Calling Dr. Fauci\\nOur interview with the nati...   6  \n",
       "7  Bad News Wrapped in Protein: Inside the Corona...   7  \n",
       "8  William Frankland, Pioneering Allergist, Dies ...   8  \n",
       "9  In the American South, a Perfect Storm Is Gath...   9  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_model = CorefModel()\n",
    "coref_model.import_dataset('/Users/clementineabed-meraim/Documents/Stage 2021 Medialab/data/essai.csv', colnames=['text','text_comp'])\n",
    "coref_model.clean()\n",
    "coref_model.__transform_neuralcoref__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusters</th>\n",
       "      <th>doc_key</th>\n",
       "      <th>sentences</th>\n",
       "      <th>speakers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[It’s, High, Time, We, Fought, This, Virus, t...</td>\n",
       "      <td>[[, , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[Carlota’s, World:, What, Children, Can, Teac...</td>\n",
       "      <td>[[, , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[Does, Covid, -19, Hit, Women, and, Men, Diff...</td>\n",
       "      <td>[[, , , , , , , ], [, , , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[What, We, Pretend, to, Know, About, the, Cor...</td>\n",
       "      <td>[[, , , , , , , , , , , , , , , , , , ], [, , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[Covid, -19:, A, Look, Back, From, 2025, In, ...</td>\n",
       "      <td>[[, , , , , , , , , , , , , , ], [], [, , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[Can, an, Old, Vaccine, Stop, the, New, Coron...</td>\n",
       "      <td>[[, , , , , , , , ], [, , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[Calling, Dr., Fauci, Our, interview, with, t...</td>\n",
       "      <td>[[, , , , , , , , , , , ], [, , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[Bad, News, Wrapped, in, Protein:, Inside, th...</td>\n",
       "      <td>[[, , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[William, Frankland,, Pioneering, Allergist,,...</td>\n",
       "      <td>[[, , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[]</td>\n",
       "      <td>nw</td>\n",
       "      <td>[[In, the, American, South,, a, Perfect, Storm...</td>\n",
       "      <td>[[, , , , , , , , , , , , , , , , , , , , , , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  clusters doc_key                                          sentences  \\\n",
       "0       []      nw  [[It’s, High, Time, We, Fought, This, Virus, t...   \n",
       "1       []      nw  [[Carlota’s, World:, What, Children, Can, Teac...   \n",
       "2       []      nw  [[Does, Covid, -19, Hit, Women, and, Men, Diff...   \n",
       "3       []      nw  [[What, We, Pretend, to, Know, About, the, Cor...   \n",
       "4       []      nw  [[Covid, -19:, A, Look, Back, From, 2025, In, ...   \n",
       "5       []      nw  [[Can, an, Old, Vaccine, Stop, the, New, Coron...   \n",
       "6       []      nw  [[Calling, Dr., Fauci, Our, interview, with, t...   \n",
       "7       []      nw  [[Bad, News, Wrapped, in, Protein:, Inside, th...   \n",
       "8       []      nw  [[William, Frankland,, Pioneering, Allergist,,...   \n",
       "9       []      nw  [[In, the, American, South,, a, Perfect, Storm...   \n",
       "\n",
       "                                            speakers  \n",
       "0  [[, , , , , , , , , , , , , , , , , , , , , , ...  \n",
       "1  [[, , , , , , , , , , , , , , , , , , , , , , ...  \n",
       "2  [[, , , , , , , ], [, , , , , , , , , , , , , ...  \n",
       "3  [[, , , , , , , , , , , , , , , , , , ], [, , ...  \n",
       "4  [[, , , , , , , , , , , , , , ], [], [, , , , ...  \n",
       "5  [[, , , , , , , , ], [, , , , , , , , , , , , ...  \n",
       "6  [[, , , , , , , , , , , ], [, , , , , , , , , ...  \n",
       "7  [[, , , , , , , , , , , , , , , , , , , , , , ...  \n",
       "8  [[, , , , , , , , , , , , , , , , , , , , , , ...  \n",
       "9  [[, , , , , , , , , , , , , , , , , , , , , , ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_model.__transform_e2ecoref__(col='text')\n",
    "coref_model.__transform_e2ecoref__(col='text_comp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clusters_text</th>\n",
       "      <th>text_comp</th>\n",
       "      <th>clusters_text_comp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Governors and health officials tell us that th...</td>\n",
       "      <td>[((us), (us), (we), (we)), ((The, federal, gov...</td>\n",
       "      <td>It’s High Time We Fought This Virus the Americ...</td>\n",
       "      <td>[((The, administration), (it)), ((the, Defense...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everything seems normal. And yet nothing is. I...</td>\n",
       "      <td>[((the, same, house, as, Carlota, ,, a, wonder...</td>\n",
       "      <td>Carlota’s World: What Children Can Teach Us Du...</td>\n",
       "      <td>[((Carlota), (Carlota), (she), (her), (her), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>— Caroline Criado Perez, author of “Invisible ...</td>\n",
       "      <td>[((the, U.S.), (the, U.S.), (the, U.S.)), ((th...</td>\n",
       "      <td>Does Covid -19 Hit Women and Men Differently? ...</td>\n",
       "      <td>[((the, U.S.), (the, U.S.), (the, U.S.)), ((th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But we seem to be living in a nightmare scenar...</td>\n",
       "      <td>[((The, coronavirus), (it), (It), (it)), ((the...</td>\n",
       "      <td>What We Pretend to Know About the Coronavirus ...</td>\n",
       "      <td>[((We), (Us)), ((the, Coronavirus), (The, coro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When Covid -19 first emerged as a health crisi...</td>\n",
       "      <td>[((authoritarian, regimes), (their), (their), ...</td>\n",
       "      <td>Covid -19: A Look Back From 2025\\nIn which the...</td>\n",
       "      <td>[((authoritarian, regimes), (their), (their), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The vaccine seems to “train” the immune system...</td>\n",
       "      <td>[((The, vaccine), (the, vaccine)), ((the, coro...</td>\n",
       "      <td>Can an Old Vaccine Stop the New Coronavirus ?\\...</td>\n",
       "      <td>[((the, New, Coronavirus), (the, coronavirus),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Negotiations to schedule a time with him were ...</td>\n",
       "      <td>[((him), (His), (he), (him)), ((His, staff), (...</td>\n",
       "      <td>Calling Dr. Fauci\\nOur interview with the nati...</td>\n",
       "      <td>[((Our), (our), (our)), ((’s, show[https://www...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Research on other coronaviruses has given scie...</td>\n",
       "      <td>[((RNA), (the, RNA), (the, virus, RNA), (RNA),...</td>\n",
       "      <td>Bad News Wrapped in Protein: Inside the Corona...</td>\n",
       "      <td>[((scientists), (They)), ((RNA), (the, RNA), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dr. William Frankland, one of the top allergis...</td>\n",
       "      <td>[((London), (London)), ((His), (He), (his), (H...</td>\n",
       "      <td>William Frankland, Pioneering Allergist, Dies ...</td>\n",
       "      <td>[((William, Frankland, ,, Pioneering, Allergis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>But it had to be done. Nashville’s courts and ...</td>\n",
       "      <td>[((Nashville), (Nashville), (Nashville)), ((se...</td>\n",
       "      <td>In the American South, a Perfect Storm Is Gath...</td>\n",
       "      <td>[((NASHVILLE), (Nashville), (Nashville), (Nash...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Governors and health officials tell us that th...   \n",
       "1  Everything seems normal. And yet nothing is. I...   \n",
       "2  — Caroline Criado Perez, author of “Invisible ...   \n",
       "3  But we seem to be living in a nightmare scenar...   \n",
       "4  When Covid -19 first emerged as a health crisi...   \n",
       "5  The vaccine seems to “train” the immune system...   \n",
       "6  Negotiations to schedule a time with him were ...   \n",
       "7  Research on other coronaviruses has given scie...   \n",
       "8  Dr. William Frankland, one of the top allergis...   \n",
       "9  But it had to be done. Nashville’s courts and ...   \n",
       "\n",
       "                                       clusters_text  \\\n",
       "0  [((us), (us), (we), (we)), ((The, federal, gov...   \n",
       "1  [((the, same, house, as, Carlota, ,, a, wonder...   \n",
       "2  [((the, U.S.), (the, U.S.), (the, U.S.)), ((th...   \n",
       "3  [((The, coronavirus), (it), (It), (it)), ((the...   \n",
       "4  [((authoritarian, regimes), (their), (their), ...   \n",
       "5  [((The, vaccine), (the, vaccine)), ((the, coro...   \n",
       "6  [((him), (His), (he), (him)), ((His, staff), (...   \n",
       "7  [((RNA), (the, RNA), (the, virus, RNA), (RNA),...   \n",
       "8  [((London), (London)), ((His), (He), (his), (H...   \n",
       "9  [((Nashville), (Nashville), (Nashville)), ((se...   \n",
       "\n",
       "                                           text_comp  \\\n",
       "0  It’s High Time We Fought This Virus the Americ...   \n",
       "1  Carlota’s World: What Children Can Teach Us Du...   \n",
       "2  Does Covid -19 Hit Women and Men Differently? ...   \n",
       "3  What We Pretend to Know About the Coronavirus ...   \n",
       "4  Covid -19: A Look Back From 2025\\nIn which the...   \n",
       "5  Can an Old Vaccine Stop the New Coronavirus ?\\...   \n",
       "6  Calling Dr. Fauci\\nOur interview with the nati...   \n",
       "7  Bad News Wrapped in Protein: Inside the Corona...   \n",
       "8  William Frankland, Pioneering Allergist, Dies ...   \n",
       "9  In the American South, a Perfect Storm Is Gath...   \n",
       "\n",
       "                                  clusters_text_comp  \n",
       "0  [((The, administration), (it)), ((the, Defense...  \n",
       "1  [((Carlota), (Carlota), (she), (her), (her), (...  \n",
       "2  [((the, U.S.), (the, U.S.), (the, U.S.)), ((th...  \n",
       "3  [((We), (Us)), ((the, Coronavirus), (The, coro...  \n",
       "4  [((authoritarian, regimes), (their), (their), ...  \n",
       "5  [((the, New, Coronavirus), (the, coronavirus),...  \n",
       "6  [((Our), (our), (our)), ((’s, show[https://www...  \n",
       "7  [((scientists), (They)), ((RNA), (the, RNA), (...  \n",
       "8  [((William, Frankland, ,, Pioneering, Allergis...  \n",
       "9  [((NASHVILLE), (Nashville), (Nashville), (Nash...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_model.inference(model = 'neuralcoref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It’s High Time We Fought This Virus the American Way\\n\\x1b[38;5;0m\\x1b[48;5;240mThe administration\\x1b[0;0m has all the authority \\x1b[38;5;0m\\x1b[48;5;240mit\\x1b[0;0m needs to produce medical supplies and prepare for a potential vaccine . Every Marine knows better than to pull a knife in a gunfight. But so far, that appears to be the federal government’s approach to battling Covid -19. The president has “invoked” the Defense Production Act, but the government has not used the full authority[https://www.nytimes.com/2020/03/31/us/politics/coronavirus-defense-production-act.html] of the act. There is a difference between invokinga law and usingit, just as there is a difference between talk and action.\\nGovernors and health officials tell us that there is a profound gap between the protective equipment, hospital equipment and testing resources that are needed (and will be needed) and what is available (or in the pipeline). Bill Gates reminds us that we will need to produce millions, perhaps billions, of doses of vaccine [https://www.washingtonpost.com/opinions/bill-gates-heres-how-to-make-up-for-lost-time-on-covid-19/2020/03/31/ab5c3cf2-738c-11ea-85cb-8670579b863d_story.html] in 12 to 18 months. This isn’t a passing crisis; we will need more of everything in two months, six months and maybe years. Don’t let debate over the details of General Motors ’ and Ventec’s honorable effort to build more ventilators[https://www.nytimes.com/2020/03/30/business/gm-ventilators-coronavirus-trump.html] hide the bottom line: The federal government has all the authority it needs to close the supply gap, allocate resources among states, and prepare for the production and distribution of the vaccine to come. Until the federal government demonstrates — with statistics, contracts and timelines — that the gap is closed and the vaccine pipeline is ready, we should ask: Why isn’t the government bringing its full arsenal to the fight? The D.P.A.’s authorities[https://fas.org/sgp/crs/natsec/IN11231.pdf] go beyond prioritizing contracts and manufacturing supplies. Its allocation authority addresses the problem of states’ competing against one another for scarce resources based on market mechanisms. The federal government can allocate equipment and supplies based on actual need and best public-health practices. The D.P.A.’s industry assessment authority can be used to measure production and distribution capacity, remove blind spots, plan efficiently and recreate a supply chain at home. The federal government can determine now which entities could produce vaccines while it plans for their ethical allocation. The government can then use the D.P.A.’s Title III incentive authorities to issue loans, offer antitrust protection and guarantee purchases, creating a secure market for masks, tests and vaccines . The law is so broad in places that it is sometimes referred to as a “commandeering” authority. Lawyers prefer to say the president would act at the zenith of his authority under the paradigm presented in Justice Jackson’s concurrence[https://www.law.cornell.edu/supremecourt/text/343/579] in the Supreme Court’s landmark Youngstown[https://supreme.justia.com/cases/federal/us/343/579/] https://supreme.justia.com/cases/federal/us/343/579/[https://supreme.justia.com/cases/federal/us/343/579/] case[https://supreme.justia.com/cases/federal/us/343/579/]. But its use is not as extraordinary as some suggest, and it is not commandeering. The Defense Department alone uses the prioritization authority some 300,000 times a year, while the government uses Title III incentives 20 to 30 times per year. Although the allocation authority has not been used since the Cold War[https://www.fema.gov/media-library-data/1582898704576-dc44bbe61cce3cf763cc8a6b92617188/2018_DPAC_Report_to_Congress.pdf], some civilian airliners and freighters remain allocated for the Civil Reserve Air Fleet. Nor does exercise of the prioritization and allocation authority equate to state ownership. Under the act, corporations are paid fair market value for their products. Any actual “commandeering” of production would require just compensation under the Fifth Amendment’s Takings Clause. Moreover, price controls under the D.P.A. require a joint resolution of Congress signed into law by the president. The D.P.A. also contains built-in safeguards. A majority of its provisions expire every five years. That is why it has been reauthorized by Congress over 50 times[https://crsreports.congress.gov/product/pdf/R/R43767#page=6] since its passage in 1950. Congress can decline to do so or do so with amendments when the act comes up for reauthorization in 2025, or sooner. The law also gives federal courts jurisdiction over disputes including the power to determine and award fair market value and forbid the executive branch from using the statute improperly. If a business feels the government is overreaching or unrealistic in its timelines, it can seek relief in court and do so on an emergency basis. Finally, the government reports annually on the D.P.A.’s use: Not only will the Congress know how the act is used, the public will, too. What about liability? The first answer is to make equipment that works. The second answer is Title 42, giving the secretary of health and human services authority to determine that the coronavirus pandemic “constitutes a public health emergency[https://uscode.house.gov/view.xhtml?req=(title:42%20section:247d-6d%20edition:prelim)],” recommend use of “covered countermeasures” to fight the pandemic and in doing so grant broad protection from liability under both state and federal law, except in the case of “willful misconduct.” There are other solutions as well, like special legislation, indemnification clauses to contracts, the government contract defense and, if all else fails, waivers. Here’s the big picture: In times of crisis, when lives are at stake, lawyers and policymakers are supposed to find solutions to problems — to get to yes with honor and within the law — and not to create obstacles. State and local authorities are imploring the federal government to use the authority it has to secure our medical supply chain. So far, the administration appears to have responded like a parent doling out candy to a child: one piece at a time. This is an “all hands on deck” moment, not merely to flatten the curve but to leap ahead of the curve. America was once the arsenal of democracy; the D.P.A. can help make us the arsenal of public health. If I were advising the president (or the secretaries with delegated authority), I would say this: Please, tell the public what the need is and how the need will be met today, next week and in the months to come. What specifically has been contracted for, in how many units and on what timeline? Where there is a gap between need and supply, use the D.P.A. to close it. I’ve never heard of a commander who complained about having too many tanks or who asked for a few artillery shells and not one too many. It’s high time we fought the virus the American way: with everything we’ve got. James E. Baker, the director of Syracuse University’s Institute for Security Policy and Law, is a former Marine infantry officer, legal adviser to the National Security Council and chief judge of the U.S. Court of Appeals for the Armed Forces. The Times is committed to publishing a diversity of letters[https://www.nytimes.com/2019/01/31/opinion/letters/letters-to-editor-new-york-times-women.html] to the editor. We’d like to hear what you think about this or any of our articles. Here are some tips[https://help.nytimes.com/hc/en-us/articles/115014925288-How-to-submit-a-letter-to-the-editor]. And here’s our email: letters@nytimes.com[mailto:letters@nytimes.com]. Follow The New York Times Opinion section on Facebook[https://www.facebook.com/nytopinion], Twitter (@NYTopinion), Twitter (@NYTopinion)[http://twitter.com/NYTOpinion] and Instagram[https://www.instagram.com/nytopinion/].'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_model.visualisation(model = 'neuralcoref',col='text_comp',i=0)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a657daffbadbb687420035ea6cc897d4282c9e980605e2a6478a41911dd8a66"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ncorrefEnv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
