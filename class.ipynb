{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tempfile\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import spacy\n",
    "import logging;\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import neuralcoref\n",
    "nlp = spacy.load('en')\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "import colorama\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "\n",
    "class CorefModel:\n",
    "    def __init__(self):\n",
    "        return None\n",
    "\n",
    "\n",
    "    def import_dataset(self,path,colnames):\n",
    "        \"\"\"\n",
    "        Import the dataset of interest, check if colnames is at the right format and set dataset and colnames attributes\n",
    "        Args:\n",
    "            path: string \n",
    "                pathfile of the dataset the will be used for coreference detection\n",
    "            colnames: str or list of str if multiple columns\n",
    "                columns of the dataset for which we want to predict coreference chain\n",
    "        Returns:\n",
    "            df: dataset\n",
    "        \"\"\"   \n",
    "        df = pd.read_csv(path)\n",
    "        self.df = df\n",
    "        # Check if the columns are at the right format and set attribute colnames\n",
    "        if type(colnames) == list :\n",
    "            self.colnames = colnames\n",
    "        else : \n",
    "            if type(colnames)==str:\n",
    "                self.colnames = [colnames]  \n",
    "            else :\n",
    "                print('Argument colnames is not a list of string or a string')\n",
    "                raise TypeError      \n",
    "\n",
    "        return self.df\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset before\n",
    "        Check if the columns of interest are strings and prepocess the columns\n",
    "        Returns:\n",
    "            df: dataset\n",
    "        \"\"\"   \n",
    "        for col in self.colnames :\n",
    "            # Check if columns are strings \n",
    "            if self.df.dtypes[col] == str :\n",
    "                # Replace wrong typos\n",
    "                self.df[col] = self.df[col].str.replace('\\n','. ')\n",
    "                self.df[col] = self.df[col].str.replace('  ',' ')\n",
    "            elif self.df.dtypes[col] == object :\n",
    "                self.df[col] = self.df[col].astype(str)\n",
    "                self.df[col] = self.df[col].str.replace('\\n','. ')\n",
    "                self.df[col] = self.df[col].str.replace('  ',' ')\n",
    "            else :\n",
    "                print(f\"Column type {col} is not string\")\n",
    "                raise TypeError\n",
    "            return self.df\n",
    "    \n",
    "\n",
    "    def __transform_neuralcoref__(self):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset and clean before\n",
    "        Set the dataset to an adapted form for evaluation using NeuralCoref model\n",
    "        Returns: \n",
    "            df_eval: dataset \n",
    "        \"\"\" \n",
    "        # NeuralCoref only needs dataset with the columns of interest\n",
    "        self.df_eval = self.df\n",
    "        return self.df_eval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __split_into_sentences__(self, text):\n",
    "        \"\"\" \n",
    "        -- Sub-function : step of __transform_e2ecoref__ --\n",
    "        Split a text into a list of sentences\n",
    "        Args: \n",
    "            text : str\n",
    "        Returns:\n",
    "            sentences : list of str \n",
    "                list of the text sentences\n",
    "        \"\"\"\n",
    "\n",
    "        # Typos needed\n",
    "        alphabets= \"([A-Za-z])\"\n",
    "        prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "        suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "        starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "        acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "        websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "        # Fixe the typos\n",
    "        text = \" \" + text + \"  \"\n",
    "        text = text.replace(\"\\n\",\" \")\n",
    "        text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "        text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "        if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "        text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "        text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "        text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "        text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "        text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "        text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "        text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "        if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "        if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "        if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "        if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "        text = text.replace(\".\",\".<stop>\")\n",
    "        text = text.replace(\"?\",\"?<stop>\")\n",
    "        text = text.replace(\"!\",\"!<stop>\")\n",
    "        text = text.replace(\"<prd>\",\".\")\n",
    "\n",
    "        # Split into sentences\n",
    "        sentences = text.split(\"<stop>\")\n",
    "        sentences = sentences[:-1]\n",
    "        sentences = [s.strip() for s in sentences]\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "    def __formatage_liste__(self, text):\n",
    "        \"\"\" \n",
    "        -- Sub-function : step of __transform_e2ecoref__ --\n",
    "        Split the text into a list of list of words for each sentence and associate it a list of list of speakers for each word\n",
    "        \n",
    "        Args: \n",
    "            text : str\n",
    "        Returns:\n",
    "            [liste_formate, liste_speaker] : list of two lists\n",
    "                liste_formate: list of lists \n",
    "                    list of words for each sentence of a text\n",
    "                liste_speaker: list of lists \n",
    "                    speaker for each word of liste_formate (empty strings by default)\n",
    "        \"\"\"\n",
    "        liste_formate = []\n",
    "        liste_speaker = []\n",
    "\n",
    "        # Create a list of sentences\n",
    "        liste_sentence = self.__split_into_sentences__(text)\n",
    "\n",
    "        # Transform each sentence to a list of words and create a list of empty strings\n",
    "        for sentence in liste_sentence :\n",
    "            liste_mot = list(sentence.split(\" \")) \n",
    "            liste_speak = [\"\" for i in liste_mot]\n",
    "\n",
    "            # Add them to the \"global\" list\n",
    "            liste_formate.append(liste_mot)\n",
    "            liste_speaker.append(liste_speak)\n",
    "\n",
    "        return [liste_formate, liste_speaker]\n",
    "\n",
    "\n",
    "    def __dico__(self, text):\n",
    "        \"\"\" \n",
    "        -- Sub-function : step of __transform_e2ecoref__ --\n",
    "        Create a dictionnary for a text under a specific format, needed to use e2ecoref\n",
    "\n",
    "        Args: \n",
    "            text : str\n",
    "        Returns:\n",
    "            dico : dictionnary\n",
    "                dictionnary for the text under the following format :\n",
    "                    - clusters : where the coreference chains of the text will be added\n",
    "                    - doc_key : genre of the text, set by default to \"nw\" news wire, but can be changed (\"bc\": broadcast conversations, \n",
    "                    \"bn\": broadcast news, \"mz\": magazines, \"nw\": news wire, \"pt\": pivot corpus, \"tc\": telephone conversation, \"wb\": weblogs)\n",
    "                    - sentences : list of the text sentences, each being under a list of words format\n",
    "                    - speakers : speaker for each word, respecting the same format as previous \"sentences\" \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        dico={\n",
    "        \"clusters\": [],\n",
    "        \"doc_key\": \"nw\",\n",
    "        \"sentences\": self.__formatage_liste__(text)[0],\n",
    "        \"speakers\": self.__formatage_liste__(text)[1]\n",
    "        }\n",
    "\n",
    "        return dico\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __transform_e2ecoref__(self,col):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset and clean before\n",
    "        For one specific column, create a json file to an adapted form for inference with e2eCoref model. Can be used successively \n",
    "        if there are several column of interest for one dataset.\n",
    "        Args:\n",
    "            col : str\n",
    "                name of the specific column for which we want to use the model\n",
    "        Returns: \n",
    "            df_eval : dataset \n",
    "                dataset of the json file with the following columns :\n",
    "                    - clusters : where the coreference chains of the text will be added\n",
    "                    - doc_key : genre of the text, set by default to \"nw\" news wire, but can be changed (\"bc\": broadcast conversations, \n",
    "                    \"bn\": broadcast news, \"mz\": magazines, \"nw\": news wire, \"pt\": pivot corpus, \"tc\": telephone conversation, \"wb\": weblogs)\n",
    "                    - sentences : list of the text sentences, each being under a list of words format\n",
    "                    - speakers : speaker for each word, respecting the same format as previous \"sentences\" \n",
    "\n",
    "        \"\"\"  \n",
    "\n",
    "        # Create the dictionnary  \n",
    "        dicos_list = []\n",
    "\n",
    "        # For each text of the column create a dico under the right format\n",
    "        for text in self.df[col]:\n",
    "            dicos_list.append(self.__dico__(text))\n",
    "        print(dicos_list)\n",
    "        # Saving the dataset under a json temporary file \n",
    "        self.fp1 = tempfile.NamedTemporaryFile(mode='ab')\n",
    "        datapath = self.fp1.name\n",
    "        print(datapath)\n",
    "        # For each line of the dictionnary\n",
    "        for dico in dicos_list :\n",
    "            print(type(dico))\n",
    "            print(dico)\n",
    "            self.fp1.write(bytes(json.dumps(dico), 'utf-8'))\n",
    "            self.fp1.write(b'\\n')\n",
    "        \n",
    "        # Read the saved json temporary file \n",
    "        df_eval = pd.read_json(datapath, orient='records', lines=True, encoding='utf-8')\n",
    "        \n",
    "        return df_eval\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __neuralcoref__(self,col):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset, clean and __transform_neuralcoref__ before\n",
    "        Gives the coreference chain clusters of each text for column col of the dataset, using the NeuralCoref model \n",
    "\n",
    "        Args:\n",
    "            col : str\n",
    "                name of the specific column for which we want to use the model\n",
    "        Returns: \n",
    "            column_coref : list of list of lists of spans\n",
    "                future dataset column, each element i being a list of every coreference clusters \n",
    "                found by the model for text line i of the dataset. A coreference cluster is a list of \n",
    "                text \"spans\" (specific class used by NeuralCoref model)\n",
    "        \"\"\" \n",
    "\n",
    "        column_coref = []\n",
    "        # For each text of the dataset\n",
    "        for i in range(len(self.df_eval)):\n",
    "            text = self.df_eval[col][i]\n",
    "            text_nlp = nlp(text)\n",
    "            # Use neuralcoref module to give the coreference chains clusters of the text\n",
    "            column_coref.append(text_nlp._.coref_clusters)\n",
    "\n",
    "        return column_coref\n",
    "            \n",
    " \n",
    "    def __e2ecoref__(self,col):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset, clean and __transform_e2ecoref__ before\n",
    "        Gives the dataframe presenting results of coreference detection using e2eCoref model for one specific column of text.\n",
    "        Args: \n",
    "            col : str \n",
    "                name of the specific column for which we want to use the model\n",
    "        Returns: \n",
    "            df_coref[col] : dataframe \n",
    "                dataframe with the following columns :\n",
    "                    (each column line corresponding to the evaluation of the text of the dataframe line i)\n",
    "                    - clusters : empty lists\n",
    "                    - doc key : \"nw\" by default (news wire)\n",
    "                    - sentences : list of lists, each sentences of the text splitted into list of words\n",
    "                    - speakers : list of lists, gives speakers for each word, following previous \"sentences\" format\n",
    "                    - predicted_clusters : list of lists, for each clusters of coreference chain found gives list of mention positions \n",
    "        \"\"\"\n",
    "\n",
    "        # Call the temporary file used for method __transform_e2ecoref__ (which create the file used for evaluation)\n",
    "        datapath = self.fp1.name \n",
    "        # Create new temporary file for evaluation output\n",
    "        self.fp2 = tempfile.NamedTemporaryFile(encoding='utf-8')\n",
    "        output = self.fp2.name\n",
    "\n",
    "        # Prediction using e2eCoref\n",
    "        os.system(f'python predict.py final {datapath} {output}')\n",
    "        self.fp1.close()\n",
    "\n",
    "        df_coref = pd.read_json(output, orient='records', lines=True)\n",
    "        self.df_coref[col] = df_coref\n",
    "        self.fp2.close()\n",
    "\n",
    "        return self.df_coref[col]\n",
    "\n",
    "\n",
    "    def inference(self,model):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset and clean before\n",
    "        Arguments : self, model (NeuralCoref or e2eCoref)\n",
    "        Detect and extract coreference chains for each text and column of the dataframe and present the results in a new dataframe.\n",
    "        \n",
    "        If model is e2ecoref, also create a dataframe used later for standardized results :\n",
    "        df_useful : dataframe\n",
    "             for each column col : \n",
    "                - text_list_col : text under list format\n",
    "                - predicted_clusters_col : coreference mentions positions under list format\n",
    "        \n",
    "        Args: \n",
    "            model : str \n",
    "                name of the model we want to use for coreference detection (neuralcoref or e2ecoref)\n",
    "        Returns: \n",
    "            df_results : dataframe \n",
    "\n",
    "            if model is neuralcoref :\n",
    "                dataframe with the following columns, for each column col :\n",
    "                    (each column line corresponding to the evaluation of the text of the dataframe line i)\n",
    "                    - col : texts of the original dataframe\n",
    "                    - clusters_col : list of lists of spans, for each clusters of coreference chain gives list of spans\n",
    "                    (this specific class used by NeuralCoref contains in itself the span start and end positions) \n",
    "\n",
    "            if model is e2coref :\n",
    "                dataframe with the following columns, for each column col :\n",
    "                    (each column line corresponding to the evaluation of the text of the dataframe line i)\n",
    "                    - col : texts of the original dataframe\n",
    "                    - clusters_col : list of lists of strings, for each clusters of coreference chain gives list of mentions\n",
    "        \"\"\"\n",
    "\n",
    "        # Create dataframe for results \n",
    "        self.df_results = pd.DataFrame()\n",
    "\n",
    "        # According to the model given in argument, create columns of results\n",
    "\n",
    "        if model == \"neuralcoref\":\n",
    "            # Run the method required before using the methode __neuralcoref__\n",
    "            self.df_eval = self.__transform_neuralcoref__()\n",
    "\n",
    "            for col in self.colnames :\n",
    "                # Column of col texts\n",
    "                self.df_results[col] = self.df[col]\n",
    "                # Column of clusters found for each col\n",
    "                self.df_results[f'clusters_{col}'] = self.__neuralcoref__(col)\n",
    "\n",
    "        elif model == \"e2ecoref\" :\n",
    "            # Create dataframe for further use \n",
    "            self.df_useful = pd.DataFrame()\n",
    "\n",
    "            # For each column of colnames\n",
    "            for col in self.colnames :\n",
    "                # Run methods required \n",
    "                df_eval = self.__transform_e2ecoref__(col)\n",
    "                df_coref = self.__e2ecoref__(col)\n",
    "\n",
    "                # Column of col texts\n",
    "                self.df_results[col] = self.df[col]\n",
    "            \n",
    "                # Give the mention string of the mentions positions given by column predicted_clusters\n",
    "                column_cluster = []\n",
    "                column_text_list = []\n",
    "\n",
    "                for i in range(len(self.df_results)):\n",
    "                    liste_clusters = []\n",
    "                    list_clusters_num = df_coref[\"predicted_clusters\"][i]\n",
    "                    list_sentences = df_coref['sentences'][i]\n",
    "                    # Text under list format\n",
    "                    flat_list_sentences = [item for sublist in list_sentences for item in sublist]\n",
    "\n",
    "                    for cluster in list_clusters_num :\n",
    "                        # List of the cluster strings\n",
    "                        cluster_str = []\n",
    "                        # For each mention of the coreference chain\n",
    "                        for item in cluster :\n",
    "                            mention_start = item[0]\n",
    "                            mention_end = item[1] + 1\n",
    "                            # Add the mention string to the cluster of strings\n",
    "                            cluster_str.append(flat_list_sentences[mention_start:mention_end])\n",
    "\n",
    "                        # Add the cluster of strings to the list of clusters\n",
    "                        liste_clusters.append(cluster_str)\n",
    "\n",
    "                    # Add the list of clusters of text line i to the column\n",
    "                    column_cluster.append(liste_clusters)\n",
    "                    # Add the text under list format to the column\n",
    "                    column_text_list.append(flat_list_sentences)\n",
    "\n",
    "                # Column of col texts\n",
    "                self.df_results[f'clusters_{col}'] = column_cluster\n",
    "\n",
    "                # Column of text_list add to useful dataframe\n",
    "                self.df_useful[f'text_list_{col}'] = column_text_list\n",
    "                # Column of predicted_clusters (mentions positions under list format) add to useful dataframe\n",
    "                self.df_useful[f'predicted_clusters_{col}'] = df_coref[\"predicted_clusters\"]\n",
    "                \n",
    "        else:\n",
    "            print('This model is not manageable with CorefModel')\n",
    "            raise NameError\n",
    "        \n",
    "        return self.df_results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def __isprefixe__(self,i,mot,texte): \n",
    "        \"\"\" \n",
    "         -- Sub-function : step of standardization and visualisation --\n",
    "        Check if a word has an occurrence in a text in position i\n",
    "        Args: \n",
    "            i : int\n",
    "            mot : str\n",
    "            texte : str\n",
    "        Returns: \n",
    "            B : bool\n",
    "        \"\"\"\n",
    "\n",
    "        B = True\n",
    "        j=0\n",
    "        while (j < len(mot)) and B:\n",
    "            if texte[i+j] != mot[j]:\n",
    "                B = False\n",
    "            j+= 1 \n",
    "        return B\n",
    "\n",
    "\n",
    "    def __positions_str__(self,mention_str,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give list of occurring positions of a mention in a text\n",
    "        Args: \n",
    "            mention_str : str\n",
    "            texte : str \n",
    "        Returns: \n",
    "            occ : list \n",
    "                list of the occurring positions of the mention in the text\n",
    "        \"\"\"\n",
    "        occ = []\n",
    "        for i in range(len(texte)-len(mention_str)+1):\n",
    "            if self.__isprefixe__(i,mention_str,texte): \n",
    "                occ.append(i)\n",
    "\n",
    "        return occ\n",
    "\n",
    "\n",
    "    def __positions_span__(self, mention_str,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give list of occurring start span positions of a mention in a text\n",
    "        Args:\n",
    "            mention_str : str\n",
    "            texte : str\n",
    "        Returns:\n",
    "            occ1 : list \n",
    "                list of the start span positions of the mention in the text\n",
    "        \"\"\"\n",
    "        occ1 = []\n",
    "        for i in self.__positions_str__(mention_str,texte): \n",
    "            \n",
    "            chaine = texte[0:i+len(mention_str)]\n",
    "            mention_span = nlp(mention_str)\n",
    "            chain = nlp(chaine)\n",
    "\n",
    "            occ1.append(len(chain)-len(mention_span))\n",
    "    \n",
    "        return occ1\n",
    "\n",
    "    def __positions_list__(self,mention,texte):\n",
    "        \"\"\"\n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give list of start and end positions of a mention in a text\n",
    "        Args:\n",
    "            mention : str \n",
    "            texte : str\n",
    "        Returns: \n",
    "            occ2 : list of lists ([start,end] format)\n",
    "                list of the start and end list positions of the mention in the text\n",
    "        \"\"\"\n",
    "\n",
    "        occ2 = []\n",
    "        for i in self.__positions_str__(mention,texte): \n",
    "            \n",
    "            # To handle text part without any complete sentence\n",
    "            chaine = texte[:i] +  \".\"\n",
    "\n",
    "            liste = self.__formatage_liste__(chaine)[0]\n",
    "\n",
    "            # Transform list of lists to a simple list\n",
    "            liste_flat = [item for sublist in liste for item in sublist]\n",
    "\n",
    "            # Handle empty list\n",
    "            if liste_flat == [] :\n",
    "                liste_flat = chaine.split(\" \")\n",
    "            liste_flat.pop()\n",
    "           \n",
    "            mention_list = mention.split(\" \")\n",
    "            position = len(liste_flat) \n",
    "            occ2.append([position,position+len(mention_list)])\n",
    " \n",
    "        return occ2\n",
    "\n",
    "\n",
    "    def __position_span_to_str__(self,mention,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give the start string position of a mention in a text based on its span positions\n",
    "        Args: \n",
    "            mention : span\n",
    "            texte : str \n",
    "        Returns: \n",
    "            position_finale : int\n",
    "                start string position of the mention in the text\n",
    "        \"\"\"\n",
    "\n",
    "        mention_str = mention.text\n",
    "\n",
    "        span_position = mention.start \n",
    "\n",
    "        # Function returning the list of string positions of the mention in the text\n",
    "        liste_pos_str = self.__positions_str__(mention_str,texte) \n",
    "        # Function returning the list of span positions of the mention in the text\n",
    "        liste_pos_span = self.__positions_span__(mention_str,texte) \n",
    "        \n",
    "        # Check if the span position of the mention is in the list of span positions of the mention in the text\n",
    "        if span_position in liste_pos_span :\n",
    "            # Take the list index of that span positions\n",
    "            ind = liste_pos_span.index(span_position)\n",
    "            # Take the parallel string position corresponding to that index\n",
    "            position_finale = liste_pos_str[ind]\n",
    "    \n",
    "        return position_finale \n",
    "\n",
    "    def __position_str_to_span__(self,start,end,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give start and end span positions of a mention in a text based on its string positions\n",
    "        Args: \n",
    "            start : int\n",
    "                start str position of a mention\n",
    "            end : int\n",
    "                end str position of a mention\n",
    "            texte : str \n",
    "        Returns: \n",
    "            list ([start, end] format)\n",
    "                start and end span positions of the mention in the text\n",
    "        \"\"\"\n",
    "        mention_str = texte[start:end]\n",
    "        mention_span = nlp(mention_str)\n",
    "\n",
    "        chaine = texte[0:end]\n",
    "        chain = nlp(chaine)\n",
    "\n",
    "        return ([len(chain)-len(mention_span),len(chain)])\n",
    "\n",
    "    def __position_list_to_str__(self,position,mention,texte): \n",
    "        \"\"\" \n",
    "        -- Sub-function : step of standardization and visualisation --\n",
    "        Give start str position of a mention in a text based on its start list position\n",
    "        Args: \n",
    "            position : int\n",
    "                start position of the mention under list format\n",
    "            mention : str\n",
    "            texte : str\n",
    "        Returns: \n",
    "            position_finale : int\n",
    "                start string position of the mention in the text\n",
    "        \"\"\"\n",
    "        # Function returning the list of string positions of the mention in the text\n",
    "        liste_pos_str = self.__positions_str__(mention,texte)\n",
    "        # Function returning the list of list positions of the mention in the text\n",
    "        liste_pos_list = self.__positions_list__(mention,texte)\n",
    "\n",
    "        # Check if the list position of the mention is in the list of list positions of the mention in the text\n",
    "        if position in liste_pos_list :\n",
    "            # Take the list index of that list positions\n",
    "            ind = liste_pos_list.index(position)\n",
    "            # Take the parallel string position corresponding to that index\n",
    "            position_finale = liste_pos_str[ind]\n",
    "      \n",
    "        return position_finale \n",
    "\n",
    "\n",
    "\n",
    "    def __no_doublons__(self,clusters):\n",
    "        \"\"\" \n",
    "        -- Sub-function : step of visualisation --\n",
    "        Find every overlapping mentions of detected coreference chains for a text, \n",
    "        and only keep the one with the best coreference score\n",
    "        Args: \n",
    "            clusters : list of lists of spans\n",
    "                all clusters of coreference chain found by neuralcoref for a particular text\n",
    "        Returns: list\n",
    "                list of mentions to supress because they overlaps others \n",
    "        \"\"\"\n",
    "        liste_positions = []\n",
    "        liste_mentions = []\n",
    "        liste_mentions_a_suppr = []\n",
    "        for clust in clusters :\n",
    "            cluster = clust.mentions\n",
    "            \n",
    "        \n",
    "            for mention in cluster:\n",
    "                # List of all intervall of spans\n",
    "                liste_positions.append(pd.Interval(mention.start, mention.end))\n",
    "                # List of all spans\n",
    "                liste_mentions.append(mention) \n",
    "\n",
    " \n",
    "        # Observe if some overlaps each others\n",
    "        for interval1 in liste_positions :\n",
    "            for interval2 in liste_positions :\n",
    "                if interval1.overlaps(interval2) and interval1 != interval2 :\n",
    "\n",
    "                    i1 = liste_positions.index(interval1) \n",
    "                    i2 = liste_positions.index(interval2)\n",
    "                    mention1 = liste_mentions[i1]\n",
    "                    mention2 = liste_mentions[i2]\n",
    "\n",
    "\n",
    "                    dico1 = mention1._.coref_scores\n",
    "                    score1 = max(dico1.values())\n",
    "\n",
    "                    dico2 = mention2._.coref_scores\n",
    "                    score2 = max(dico2.values())\n",
    "                \n",
    "                    # Add the mention with the lower score to the list of mention to suppress\n",
    "                    if score1 <= score2 and [mention1.start,mention1.end] not in liste_mentions_a_suppr :\n",
    "                        liste_mentions_a_suppr.append([mention1.start, mention1.end])\n",
    "                    \n",
    "                    elif score1 > score2 and [mention2.start,mention2.end] not in liste_mentions_a_suppr :\n",
    "                        liste_mentions_a_suppr.append([mention2.start, mention2.end])\n",
    "                    \n",
    "\n",
    "        return(liste_mentions_a_suppr)\n",
    "\n",
    "\n",
    "\n",
    "    def __standardized_results__(self,model):\n",
    "        \"\"\"\n",
    "        Requires to have run the method import_dataset, clean and inference before\n",
    "        Gives a dataframe of standardized results that will be useful for visualization\n",
    "        Args: \n",
    "            model : str \n",
    "                name of the model we want to use for coreference detection (neuralcoref or e2ecoref)     \n",
    "        Returns: \n",
    "            df_standardized : dataframe \n",
    "                dataframe with the following columns, for each column col  :\n",
    "                    (each column line corresponding to the evaluation of the text of the dataframe line i)\n",
    "                    - col : texts of the original dataframe \n",
    "                    - clusters_col : list of lists, for each clusters of coreference chain gives list of mentions \n",
    "                    (if model is neuralcoref mentions are spans and if model is e2ecoref mentions are strings) \n",
    "                    - span_positions_col : list of list of lists (format [index_start,index_end]), for each clusters of coreference \n",
    "                    chain gives list of mention position under span format\n",
    "        \"\"\"\n",
    "\n",
    "        if model == \"neuralcoref\":\n",
    "            # Dataframe with columns col and clusters_col\n",
    "            self.df_standardized = self.df_results\n",
    "\n",
    "            # Build columns span_positions_col \n",
    "            for col in self.colnames :\n",
    "\n",
    "                # Create column giving the span positions of the mentions of coreference chains of each text\n",
    "                column_span_pos = []\n",
    "                for i in range(len(self.df_standardized)) :\n",
    "                    # List of lists : span positions of every mention of every cluster for one text\n",
    "                    text_span_pos = []\n",
    "\n",
    "                    # Mentions to suppress\n",
    "                    mentions_a_supp = self.__no_doublons__(self.df_standardized[f'clusters_{col}'][i])\n",
    "                    \n",
    "\n",
    "                    for clusters in self.df_standardized[f'clusters_{col}'][i]:\n",
    "                        cluster = clusters.mentions\n",
    "                        cluster = [mention for mention in cluster if [mention.start,mention.end] not in mentions_a_supp]\n",
    "                        \n",
    "                \n",
    "                        # List of span positions of every mention of one cluster\n",
    "                        cluster_span_pos = []\n",
    "                        for mention in cluster :\n",
    "                            # Mention are spans : add start and end span position to the cluster \n",
    "                            cluster_span_pos.append([mention.start, mention.end])\n",
    "\n",
    "                        # Add the cluster list of mention positions for each cluster \n",
    "                        text_span_pos.append(cluster_span_pos)\n",
    "                    \n",
    "                    # Add the list of lists for each text to the column\n",
    "                    column_span_pos.append(text_span_pos)\n",
    "\n",
    "                self.df_standardized[f'span_positions_{col}'] = column_span_pos\n",
    "\n",
    "\n",
    "        elif model == \"e2ecoref\" :\n",
    "            # Dataframe with columns col and clusters_col\n",
    "            self.df_standardized = self.df_results\n",
    "\n",
    "            # Build columns span_positions_col \n",
    "            for col in self.colnames :\n",
    "\n",
    "                # Create column giving the string positions of the mentions of coreference chains of each text\n",
    "                column_str_pos = []\n",
    "\n",
    "                for i in range(len(self.df_standardized)):\n",
    "                    # List of lists : string positions of every mention of every cluster for one text\n",
    "                    text_str_pos = []\n",
    "\n",
    "                    for cluster in self.df_standardized[f'clusters_{col}'][i] :\n",
    "                        # List of string positions of every mention of one cluster\n",
    "                        cluster_str_pos = []\n",
    "\n",
    "                        # Convert list positions to string positions\n",
    "                        for positions in cluster:\n",
    "                            start = positions[0]\n",
    "                            end = positions[1]+1\n",
    "                            positions_corr = [start,end]\n",
    "                            mention = self.df_useful[f'text_list_{col}'][i][start:end]\n",
    "                            mention_str = \" \".join(mention)\n",
    "                    \n",
    "                            texte = self.df_standardized[col][i]\n",
    "                            \n",
    "                            # Start string position of the mention\n",
    "                            pos_fin = self.__position_list_to_str__(positions_corr,mention_str,texte)\n",
    "\n",
    "                            # Add start and end string positions to the list \n",
    "                            cluster_str_pos.append([pos_fin,pos_fin+len(mention_str)])\n",
    "\n",
    "                        # Add the cluster list of mention positions for each cluster \n",
    "                        text_str_pos.append(cluster_str_pos)\n",
    "\n",
    "                    # Add the list of lists for each text to the column\n",
    "                    column_str_pos.append(text_str_pos)\n",
    "\n",
    "                self.df_useful[f'str_pos_{col}'] = column_str_pos\n",
    "\n",
    "                # Thanks to that new column, create column giving the spans positions of the mentions of coreference chains of each text\n",
    "                column_span_pos = []\n",
    "                for i in range(len(self.df_standardized)):\n",
    "                    text_span_pos = []\n",
    "                    for cluster in self.df_useful[f'str_pos_{col}'][i] :\n",
    "                        cluster_span_pos = []\n",
    "                        for pos_str in cluster :\n",
    "                            start = pos_str[0]\n",
    "                            end = pos_str[1]\n",
    "                            texte = self.df_standardized[col][i]\n",
    "                            pos = self.__position_str_to_span__(start,end,texte)\n",
    "                    \n",
    "                            cluster_span_pos.append(pos)\n",
    "                        # Add the cluster list of mention positions for each cluster \n",
    "                        text_span_pos.append(cluster_span_pos)\n",
    "\n",
    "                    # Add the list of lists for each text to the column  \n",
    "                    column_span_pos.append(text_span_pos)\n",
    "\n",
    "                self.df_standardized[f'span_positions_{col}']= column_span_pos\n",
    "            \n",
    "        else:\n",
    "            print('This model is not manageable with CorefModel')\n",
    "            raise NameError\n",
    "\n",
    "        \n",
    "        return self.df_standardized\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def visualisation(self,model,col,i):\n",
    "        \"\"\" \n",
    "        Requires to have run the method import_dataset, clean and inference before. \n",
    "        Given a text, highlights all the coreference chains detected by the chosen model. \n",
    "        This function must be printed to see the highlights in different colors.\n",
    "        Args: \n",
    "            model : str\n",
    "                name of the model we want to use for coreference detection (neuralcoref or e2ecoref)\n",
    "            col : str\n",
    "                column of interest (in colnames)\n",
    "            i : int\n",
    "                line of interest (in the column)\n",
    "        Returns:  \n",
    "            texte : str\n",
    "                text with all of it coreference chains underlined in different colors\n",
    "        \"\"\"\n",
    "        if model not in ['neuralcoref', 'e2ecoref']:\n",
    "            print('This model is not manageable with CorefModel')\n",
    "            raise NameError\n",
    "        \n",
    "        else:\n",
    "            self.df_standardized = self.__standardized_results__(model)\n",
    "            texte = self.df_standardized[col][i]\n",
    "            texte_or = texte \n",
    "            nlp_texte = nlp(texte)\n",
    "                \n",
    "            liste_charactere = [i for i in range(len(texte))]\n",
    "            liste_charactere_updated = [i for i in range(len(texte))]\n",
    "\n",
    "            # Font color\n",
    "            color = 0 \n",
    "            colors = 240 \n",
    "\n",
    "            clusters_positions = self.df_standardized[f'span_positions_{col}'][i]\n",
    "            \n",
    "            for cluster in clusters_positions :\n",
    "                if len(cluster)>1 :\n",
    "                    for positions in cluster :\n",
    "\n",
    "                        # Positions in spans\n",
    "                        mention_start = positions[0]\n",
    "                        mention_end = positions[1]\n",
    "\n",
    "                        # Mention in span\n",
    "                        mention = nlp_texte[mention_start:mention_end]\n",
    "                        # Mention in str\n",
    "                        mention_str = (nlp_texte[mention_start:mention_end]).text \n",
    "\n",
    "                        # Mention start position in strings\n",
    "                        index_position_start = self.__position_span_to_str__(mention,texte_or) \n",
    "                        position_start = liste_charactere_updated[index_position_start]\n",
    "                        # Mention end position in strings\n",
    "                        position_end = position_start+len(mention_str) \n",
    "\n",
    "                        # Text from beginning to mention\n",
    "                        deb = texte[0: position_start] \n",
    "                        # End of the text\n",
    "                        fin = texte[position_end:] \n",
    "\n",
    "                        # Rewrite the text\n",
    "                        texte = deb + f'\\033[38;5;{color}m' + f'\\x1b[48;5;{colors}m' + mention_str + '\\033[0;0m' + fin #on modifie texte en changeant la couleur de la mention\n",
    "                        add1 = len(f'\\033[38;5;{color}m') + len(f'\\x1b[48;5;{colors}m')\n",
    "                        add2 = len('\\033[0;0m')\n",
    "\n",
    "\n",
    "                        # Update positions of text element after adding add1\n",
    "                        for i in range(index_position_start,len(liste_charactere_updated)): \n",
    "                            liste_charactere_updated[i] += add1\n",
    "                        # Update positions of text element after adding add2\n",
    "                        for i in range(index_position_start+len(mention_str),len(liste_charactere_updated)): \n",
    "                            liste_charactere_updated[i] += add2\n",
    "                \n",
    "                return texte\n",
    "\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==2.1.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (1.19.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (0.8.2)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (2.0.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (1.0.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (0.9.6)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (7.0.8)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy==2.1.0) (0.2.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.0) (4.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.0) (4.62.2)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: neuralcoref in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (4.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from neuralcoref) (1.19.5)\n",
      "Requirement already satisfied: boto3 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from neuralcoref) (1.18.32)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from neuralcoref) (2.25.1)\n",
      "Requirement already satisfied: spacy>=2.1.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from neuralcoref) (2.1.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.26.6)\n",
      "Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n",
      "Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n",
      "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (2.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (1.0.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.0.12 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (0.8.2)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.62.2)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from boto3->neuralcoref) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from boto3->neuralcoref) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.22.0,>=1.21.32 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from boto3->neuralcoref) (1.21.32)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from botocore<1.22.0,>=1.21.32->boto3->neuralcoref) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.32->boto3->neuralcoref) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting en_core_web_sm==2.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 15.2 MB/s \n",
      "\u001b[?25h\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages/en_core_web_sm\n",
      "-->\n",
      "/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: colorama in /Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/lib/python3.7/site-packages (0.4.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/clementineabed-meraim/miniconda3/envs/ncorrefEnv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==2.1.0\n",
    "!pip install neuralcoref --no-binary neuralcoref\n",
    "!python -m spacy download en\n",
    "!pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>annotations</th>\n",
       "      <th>meta</th>\n",
       "      <th>annotation_approver</th>\n",
       "      <th>annotations_source</th>\n",
       "      <th>col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>Hong Kong, with a population of around 7.5 mil...</td>\n",
       "      <td>[{'label': 15, 'start_offset': 700, 'end_offse...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'label': 14, 'start_offset': 705, 'end_offse...</td>\n",
       "      <td>Nursing unions in 28 countries have filed a fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>Social distancing restrictions meant some sena...</td>\n",
       "      <td>[{'label': 14, 'start_offset': 317, 'end_offse...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'label': 14, 'start_offset': 317, 'end_offse...</td>\n",
       "      <td>Bojang, 29, is among the thousands of people t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>The history of humanity is the history of impa...</td>\n",
       "      <td>[{'label': 14, 'start_offset': 127, 'end_offse...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'label': 14, 'start_offset': 127, 'end_offse...</td>\n",
       "      <td>Three young people are taking legal action aga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>154</td>\n",
       "      <td>Associated Press Florida judge blocks state or...</td>\n",
       "      <td>[{'label': 14, 'start_offset': 228, 'end_offse...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'label': 14, 'start_offset': 228, 'end_offse...</td>\n",
       "      <td>Japan joined Israel and Morocco in barring all...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>155</td>\n",
       "      <td>“This is a serious setback in a delicate stage...</td>\n",
       "      <td>[{'label': 14, 'start_offset': 70, 'end_offset...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'label': 14, 'start_offset': 70, 'end_offset...</td>\n",
       "      <td>My sister has a dog. She loves him. She hates ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1   id  \\\n",
       "0           0             0  151   \n",
       "1           1             1  152   \n",
       "2           2             2  153   \n",
       "3           3             3  154   \n",
       "4           4             4  155   \n",
       "\n",
       "                                                text  \\\n",
       "0  Hong Kong, with a population of around 7.5 mil...   \n",
       "1  Social distancing restrictions meant some sena...   \n",
       "2  The history of humanity is the history of impa...   \n",
       "3  Associated Press Florida judge blocks state or...   \n",
       "4  “This is a serious setback in a delicate stage...   \n",
       "\n",
       "                                         annotations meta  \\\n",
       "0  [{'label': 15, 'start_offset': 700, 'end_offse...   {}   \n",
       "1  [{'label': 14, 'start_offset': 317, 'end_offse...   {}   \n",
       "2  [{'label': 14, 'start_offset': 127, 'end_offse...   {}   \n",
       "3  [{'label': 14, 'start_offset': 228, 'end_offse...   {}   \n",
       "4  [{'label': 14, 'start_offset': 70, 'end_offset...   {}   \n",
       "\n",
       "   annotation_approver                                 annotations_source  \\\n",
       "0                  NaN  [{'label': 14, 'start_offset': 705, 'end_offse...   \n",
       "1                  NaN  [{'label': 14, 'start_offset': 317, 'end_offse...   \n",
       "2                  NaN  [{'label': 14, 'start_offset': 127, 'end_offse...   \n",
       "3                  NaN  [{'label': 14, 'start_offset': 228, 'end_offse...   \n",
       "4                  NaN  [{'label': 14, 'start_offset': 70, 'end_offset...   \n",
       "\n",
       "                                                 col  \n",
       "0  Nursing unions in 28 countries have filed a fo...  \n",
       "1  Bojang, 29, is among the thousands of people t...  \n",
       "2  Three young people are taking legal action aga...  \n",
       "3  Japan joined Israel and Morocco in barring all...  \n",
       "4  My sister has a dog. She loves him. She hates ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_model = CorefModel()\n",
    "coref_model.import_dataset('/Users/clementineabed-meraim/Documents/Stage 2021 Medialab/dataframe.csv', colnames=['text','col'])\n",
    "coref_model.clean()\n",
    "coref_model.__transform_neuralcoref__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/Users/clementineabed-meraim/Documents/Stage 2021 Medialab/dataframe')\n",
    "#column = ['Nursing unions in 28 countries have filed a formal appeal with the United Nations over the refusal of the UK, EU and others to temporarily waive patents for Covid vaccines, saying this has cost huge numbers of lives in developing nations.',\n",
    "#'Bojang, 29, is among the thousands of people to have landed on Italy’s shores during the last decade. He fled dictatorship in the Gambia, witnessed horror in Libya and survived a dangerous journey across the Mediterranean. He has found solace in southern Italy, in a city whose warm embrace has enabled him and other refugees to thrive despite an EU asylum system that is stacked against them.',\n",
    "#'Three young people are taking legal action against the prime minister, accusing him of breaching his legal obligations to take “practical and effective measures” to tackle the climate crisis. They argue that Boris Johnson’s government is discriminating against the younger generation and people in the global south who will bear the brunt of the climate crisis.',\n",
    "#'Japan joined Israel and Morocco in barring all foreign travelers, and Australia delayed reopening its borders for two weeks.',\n",
    "#'My sister has a dog. She loves him. She hates it when he bites her.']\n",
    "#df['col'] = column\n",
    "#df.to_csv('dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clusters': [], 'doc_key': 'nw', 'sentences': [['Nursing', 'unions', 'in', '28', 'countries', 'have', 'filed', 'a', 'formal', 'appeal', 'with', 'the', 'United', 'Nations', 'over', 'the', 'refusal', 'of', 'the', 'UK,', 'EU', 'and', 'others', 'to', 'temporarily', 'waive', 'patents', 'for', 'Covid', 'vaccines,', 'saying', 'this', 'has', 'cost', 'huge', 'numbers', 'of', 'lives', 'in', 'developing', 'nations.']], 'speakers': [['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]}\n",
      "{'clusters': [], 'doc_key': 'nw', 'sentences': [['Bojang,', '29,', 'is', 'among', 'the', 'thousands', 'of', 'people', 'to', 'have', 'landed', 'on', 'Italy’s', 'shores', 'during', 'the', 'last', 'decade.'], ['He', 'fled', 'dictatorship', 'in', 'the', 'Gambia,', 'witnessed', 'horror', 'in', 'Libya', 'and', 'survived', 'a', 'dangerous', 'journey', 'across', 'the', 'Mediterranean.'], ['He', 'has', 'found', 'solace', 'in', 'southern', 'Italy,', 'in', 'a', 'city', 'whose', 'warm', 'embrace', 'has', 'enabled', 'him', 'and', 'other', 'refugees', 'to', 'thrive', 'despite', 'an', 'EU', 'asylum', 'system', 'that', 'is', 'stacked', 'against', 'them.']], 'speakers': [['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]}\n",
      "{'clusters': [], 'doc_key': 'nw', 'sentences': [['Three', 'young', 'people', 'are', 'taking', 'legal', 'action', 'against', 'the', 'prime', 'minister,', 'accusing', 'him', 'of', 'breaching', 'his', 'legal', 'obligations', 'to', 'take', '“practical', 'and', 'effective', 'measures”', 'to', 'tackle', 'the', 'climate', 'crisis.'], ['They', 'argue', 'that', 'Boris', 'Johnson’s', 'government', 'is', 'discriminating', 'against', 'the', 'younger', 'generation', 'and', 'people', 'in', 'the', 'global', 'south', 'who', 'will', 'bear', 'the', 'brunt', 'of', 'the', 'climate', 'crisis.']], 'speakers': [['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]}\n",
      "{'clusters': [], 'doc_key': 'nw', 'sentences': [['Japan', 'joined', 'Israel', 'and', 'Morocco', 'in', 'barring', 'all', 'foreign', 'travelers,', 'and', 'Australia', 'delayed', 'reopening', 'its', 'borders', 'for', 'two', 'weeks.']], 'speakers': [['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]}\n",
      "{'clusters': [], 'doc_key': 'nw', 'sentences': [['My', 'sister', 'has', 'a', 'dog.'], ['She', 'loves', 'him.'], ['She', 'hates', 'it', 'when', 'he', 'bites', 'her.']], 'speakers': [['', '', '', '', ''], ['', '', ''], ['', '', '', '', '', '', '']]}\n",
      "/var/folders/1z/t9z408vn54x157kx03j01f880000gn/T/tmp61ojdtoj\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicos_list = [{'clusters': [], 'doc_key': 'nw', 'sentences': [['Nursing', 'unions', 'in', '28', 'countries', 'have', 'filed', 'a', 'formal', 'appeal', 'with', 'the', 'United', 'Nations', 'over', 'the', 'refusal', 'of', 'the', 'UK,', 'EU', 'and', 'others', 'to', 'temporarily', 'waive', 'patents', 'for', 'Covid', 'vaccines,', 'saying', 'this', 'has', 'cost', 'huge', 'numbers', 'of', 'lives', 'in', 'developing', 'nations.']], 'speakers': [['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]}, {'clusters': [], 'doc_key': 'nw', 'sentences': [['Bojang,', '29,', 'is', 'among', 'the', 'thousands', 'of', 'people', 'to', 'have', 'landed', 'on', 'Italy’s', 'shores', 'during', 'the', 'last', 'decade.'], ['He', 'fled', 'dictatorship', 'in', 'the', 'Gambia,', 'witnessed', 'horror', 'in', 'Libya', 'and', 'survived', 'a', 'dangerous', 'journey', 'across', 'the', 'Mediterranean.'], ['He', 'has', 'found', 'solace', 'in', 'southern', 'Italy,', 'in', 'a', 'city', 'whose', 'warm', 'embrace', 'has', 'enabled', 'him', 'and', 'other', 'refugees', 'to', 'thrive', 'despite', 'an', 'EU', 'asylum', 'system', 'that', 'is', 'stacked', 'against', 'them.']], 'speakers': [['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]}, {'clusters': [], 'doc_key': 'nw', 'sentences': [['Three', 'young', 'people', 'are', 'taking', 'legal', 'action', 'against', 'the', 'prime', 'minister,', 'accusing', 'him', 'of', 'breaching', 'his', 'legal', 'obligations', 'to', 'take', '“practical', 'and', 'effective', 'measures”', 'to', 'tackle', 'the', 'climate', 'crisis.'], ['They', 'argue', 'that', 'Boris', 'Johnson’s', 'government', 'is', 'discriminating', 'against', 'the', 'younger', 'generation', 'and', 'people', 'in', 'the', 'global', 'south', 'who', 'will', 'bear', 'the', 'brunt', 'of', 'the', 'climate', 'crisis.']], 'speakers': [['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''], ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]}, {'clusters': [], 'doc_key': 'nw', 'sentences': [['Japan', 'joined', 'Israel', 'and', 'Morocco', 'in', 'barring', 'all', 'foreign', 'travelers,', 'and', 'Australia', 'delayed', 'reopening', 'its', 'borders', 'for', 'two', 'weeks.']], 'speakers': [['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']]}, {'clusters': [], 'doc_key': 'nw', 'sentences': [['My', 'sister', 'has', 'a', 'dog.'], ['She', 'loves', 'him.'], ['She', 'hates', 'it', 'when', 'he', 'bites', 'her.']], 'speakers': [['', '', '', '', ''], ['', '', ''], ['', '', '', '', '', '', '']]}]\n",
    "\n",
    "\n",
    "coref_model.fp3 = tempfile.NamedTemporaryFile(mode='ab')\n",
    "datapath = coref_model.fp3.name\n",
    "\n",
    "for dico in dicos_list :\n",
    "      print(dico)\n",
    "      coref_model.fp3.write(bytes(json.dumps(dico), 'utf-8'))\n",
    "      coref_model.fp3.write(b'\\n')\n",
    "\n",
    "#print(datapath)\n",
    "print(coref_model.fp3.name)\n",
    "df_eval = pd.read_json(datapath, orient='records', lines=True, encoding='utf-8')\n",
    "df_eval.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a657daffbadbb687420035ea6cc897d4282c9e980605e2a6478a41911dd8a66"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ncorrefEnv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
