{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.morphology.Morphology size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 96 from C header, got 112 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.tokens.span.Span size changed, may indicate binary incompatibility. Expected 72 from C header, got 80 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "INFO:neuralcoref:Loading model from /pbs/home/s/sdo/.neuralcoref_cache/neuralcoref\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.easycoref.coref import CorefModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Using class CorefModel()**\n",
    "\n",
    "\n",
    "The class CorefModel() implements two coreference detection models : NeuralCoref and e2eCoref. It allows one user to chose one of those two models and use it to detect and extract all coreference chains from a list of texts under a dataset format.\n",
    "\n",
    "The user must give in entry a dataset with one or several columns of texts for which we want to do the coreference detection. Then, the CorefModel() tool will detect and extract coreference chains for each text, and place those created clusters in a new column of the dataset. User can also use a visualisation tool to highlight the different coreference chains of one text found by one model.\n",
    "\n",
    "In this tutorial, we will present an example of coreference detection and extraction of a dataset of interest, using each model and each function of class CorefModel(). This tutorial will illustrate the standard use of that class.\n",
    "\n",
    "### 1. Dataset \n",
    "\n",
    "To use the class, the user must give either a csv or a json dataset in entry, with at least one column being a list of texts (under string format).\n",
    "\n",
    "The dataset df_tutorial used for this example is a csv. It has 4 lines and 2 columns of interest : 'text1' and 'text2', which are the columns of texts for which we want to detect coreference chains. \n",
    "Texts are in english and extracted from press corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hong Kong, with a population of around 7.5 mil...</td>\n",
       "      <td>Governors and health officials tell us that th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Social distancing restrictions meant some sena...</td>\n",
       "      <td>Everything seems normal. And yet nothing is. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The history of humanity is the history of impa...</td>\n",
       "      <td>— Caroline Criado Perez, author of “Invisible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Associated Press Florida judge blocks state or...</td>\n",
       "      <td>But we seem to be living in a nightmare scenar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               text1  \\\n",
       "0  Hong Kong, with a population of around 7.5 mil...   \n",
       "1  Social distancing restrictions meant some sena...   \n",
       "2  The history of humanity is the history of impa...   \n",
       "3  Associated Press Florida judge blocks state or...   \n",
       "\n",
       "                                               text2  \n",
       "0  Governors and health officials tell us that th...  \n",
       "1  Everything seems normal. And yet nothing is. I...  \n",
       "2  — Caroline Criado Perez, author of “Invisible ...  \n",
       "3  But we seem to be living in a nightmare scenar...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = './src/easycoref/data/df_tutorial.csv'\n",
    "\n",
    "df_tutorial = pd.read_csv(datapath).drop('Unnamed: 0', axis=1)\n",
    "df_tutorial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Standards steps of CorefModel() :**\n",
    "\n",
    "\n",
    "**The class steps must be done in order.**\n",
    "\n",
    "## 1. Calling the class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_model = CorefModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Preprocessing\n",
    "\n",
    "**Importation of the dataset** using it path and the name of the columns colnames.\n",
    "\n",
    "Colnames can be a string if there is only one column, or a list of string if there is one or several column of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hong Kong, with a population of around 7.5 mil...</td>\n",
       "      <td>Governors and health officials tell us that th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Social distancing restrictions meant some sena...</td>\n",
       "      <td>Everything seems normal. And yet nothing is. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The history of humanity is the history of impa...</td>\n",
       "      <td>— Caroline Criado Perez, author of “Invisible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Associated Press Florida judge blocks state or...</td>\n",
       "      <td>But we seem to be living in a nightmare scenar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              text1  \\\n",
       "0           0  Hong Kong, with a population of around 7.5 mil...   \n",
       "1           1  Social distancing restrictions meant some sena...   \n",
       "2           2  The history of humanity is the history of impa...   \n",
       "3           3  Associated Press Florida judge blocks state or...   \n",
       "\n",
       "                                               text2  \n",
       "0  Governors and health officials tell us that th...  \n",
       "1  Everything seems normal. And yet nothing is. I...  \n",
       "2  — Caroline Criado Perez, author of “Invisible ...  \n",
       "3  But we seem to be living in a nightmare scenar...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_model.import_dataset(datapath, colnames=['text1','text2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning of the dataset :** can only be done after importation.\n",
    "The columns of interest (given by colnames) will be cleaned : string format are checked, typos errors are corrected and line break are erased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Hong Kong, with a population of around 7.5 mil...</td>\n",
       "      <td>Governors and health officials tell us that th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Social distancing restrictions meant some sena...</td>\n",
       "      <td>Everything seems normal. And yet nothing is. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The history of humanity is the history of impa...</td>\n",
       "      <td>— Caroline Criado Perez, author of “Invisible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Associated Press Florida judge blocks state or...</td>\n",
       "      <td>But we seem to be living in a nightmare scenar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              text1  \\\n",
       "0           0  Hong Kong, with a population of around 7.5 mil...   \n",
       "1           1  Social distancing restrictions meant some sena...   \n",
       "2           2  The history of humanity is the history of impa...   \n",
       "3           3  Associated Press Florida judge blocks state or...   \n",
       "\n",
       "                                               text2  \n",
       "0  Governors and health officials tell us that th...  \n",
       "1  Everything seems normal. And yet nothing is. I...  \n",
       "2  — Caroline Criado Perez, author of “Invisible ...  \n",
       "3  But we seem to be living in a nightmare scenar...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coref_model.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choosing the model for the following steps\n",
    "\n",
    "\n",
    "When choosing a model steps of inference, and visualisation must be done successively, one model after the other. One must be careful because if   inference is done for both models, then visualisation for both, the visualisation function will only use the last resulting dataframes (*df_results*, *df_standardized_results*) so it will only work for the last model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **When choosing : NeuralCoref**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Inference**\n",
    "\n",
    "For evaluation, we use the function **inference** that take the model in argument and requires to have already import and clean the dataset.\n",
    "\n",
    "NeuralCoref detect and extract coreference chains. \n",
    "\n",
    "**Steps of the inference fonction :**\n",
    "\n",
    " - **Transforming the dataset format**\n",
    "\n",
    "We want the dataset to be to the right format, to use NeuralCoref.\n",
    "\n",
    "For NeuralCoref we only need the dataset with columns of interest to the right format, which is already the case after preprocessing.\n",
    "The transformation step creates *df_eval*, used for evaluation (which is directly the dataframe *df_tutorial* after preprocessing).\n",
    "\n",
    " - **Detect coreference chains**\n",
    "\n",
    "Detect and extract coreference chains for each text and column of the dataframe and present the results in a new dataframe called df_results with the columns of interested *col* and columns of predicted clusters *cluster_col*. Each line of a column is a list of detected clusters, each cluster being a list of spans (specific class of NeuralCoref) : it is the intervall of text corresponding to the mention. \n",
    "\n",
    "**Inference function returns the dataframe *df_results* with original columns of text and columns of coreference chain detected.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref_model.inference(model='neuralcoref')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Visualisation**\n",
    "\n",
    "Using inference function is useful to see the predicted clusters of coreference as list of text. But the dataframe returned can be complex to read and not really visual. To see the coreference chains of a specific text of the dataframe highlighted, we can print the function **visualisation**.\n",
    "\n",
    "This requires to have already import, clean and used inference on the dataset of interest. \n",
    "\n",
    "Function **visualisation** takes in argument the model (must be the same as the one chosed for inference), and the position of the text of interested : column col and line i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coref_model.visualisation(model='neuralcoref', col='text1', i=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **When choosing : e2eCoref**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Inference**\n",
    "\n",
    "For evaluation, we use the function **inference** that take the model in argument and requires to have already import and clean the dataset.\n",
    "\n",
    "e2eCoref detect and extract coreference chains. \n",
    "\n",
    "**Steps of the inference fonction :**\n",
    "\n",
    " - **Transforming the dataset format**\n",
    "\n",
    "We want the dataset to be to the right format, to use e2eCoref.\n",
    "\n",
    "For e2eCoref we need to create a specific jsonfile to the right format for each column of interest. \n",
    "The transformation step creates that file for each column *col*, called *df_coref_col*. \n",
    "\n",
    " - **Detect coreference chains**\n",
    "\n",
    "Detect and extract coreference chains for each text and column of the dataframe and present the results in a new dataframe called *df_results* with the columns of interested *col* and columns of predicted clusters *cluster_col*. Each line of a column is a list of detected clusters, each cluster being a list of strings (specific class of NeuralCoref). \n",
    "\n",
    "- **Creates a dataframe useful for further use**\n",
    "\n",
    "Parallel to the coreference chains detections, we create *df_useful* which stocks for each column *col*, columns *text_list_col* - text under list format - and *predicted_clusters_col* - list of clusters, each cluster being a list of coreference mentions positions under list format. \"List format\" means the interval [a,b] given as positions correspond to the mention returned when selecting the interval of text under list format  : text_list_col[a,b]. \n",
    "\n",
    "\n",
    "**Inference function returns the dataframe *df_results* with original columns of text and columns of coreference chain detected.**\n",
    "        \n",
    "\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /sps/humanum/user/sdo/easycoref/e2e-coref/coref_ops.py:11: The name tf.NotDifferentiable is deprecated. Please use tf.no_gradient instead.\n",
      "\n",
      "/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/site-packages/sklearn/utils/linear_assignment_.py:21: DeprecationWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  DeprecationWarning)\n",
      "Traceback (most recent call last):\n",
      "  File \"./predict.py\", line 21, in <module>\n",
      "    model = cm.CorefModel(config)\n",
      "  File \"/sps/humanum/user/sdo/easycoref/e2e-coref/coref_model.py\", line 27, in __init__\n",
      "    self.char_dict = util.load_char_dict(config[\"char_vocab_path\"])\n",
      "  File \"/sps/humanum/user/sdo/easycoref/e2e-coref/util.py\", line 58, in load_char_dict\n",
      "    with codecs.open(char_vocab_path, encoding=\"utf-8\") as f:\n",
      "  File \"/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/codecs.py\", line 904, in open\n",
      "    file = builtins.open(filename, mode, buffering)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'char_vocab.english.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting CUDA_VISIBLE_DEVICES to: \n",
      "Running experiment: final\n",
      "max_top_antecedents = 50\n",
      "max_training_sentences = 50\n",
      "top_span_ratio = 0.4\n",
      "filter_widths = [\n",
      "  3\n",
      "  4\n",
      "  5\n",
      "]\n",
      "filter_size = 50\n",
      "char_embedding_size = 8\n",
      "char_vocab_path = \"char_vocab.english.txt\"\n",
      "context_embeddings {\n",
      "  path = \"glove.840B.300d.txt\"\n",
      "  size = 300\n",
      "}\n",
      "head_embeddings {\n",
      "  path = \"glove_50_300_2.txt\"\n",
      "  size = 300\n",
      "}\n",
      "contextualization_size = 200\n",
      "contextualization_layers = 3\n",
      "ffnn_size = 150\n",
      "ffnn_depth = 2\n",
      "feature_size = 20\n",
      "max_span_width = 30\n",
      "use_metadata = true\n",
      "use_features = true\n",
      "model_heads = true\n",
      "coref_depth = 2\n",
      "lm_layers = 3\n",
      "lm_size = 1024\n",
      "coarse_to_fine = true\n",
      "max_gradient_norm = 5.0\n",
      "lstm_dropout_rate = 0.4\n",
      "lexical_dropout_rate = 0.5\n",
      "dropout_rate = 0.2\n",
      "optimizer = \"adam\"\n",
      "learning_rate = 0.001\n",
      "decay_rate = 0.999\n",
      "decay_frequency = 100\n",
      "train_path = \"train.english.jsonlines\"\n",
      "eval_path = \"test.english.jsonlines\"\n",
      "conll_eval_path = \"test.english.v4_gold_conll\"\n",
      "lm_path = \"\"\n",
      "genres = [\n",
      "  \"bc\"\n",
      "  \"bn\"\n",
      "  \"mz\"\n",
      "  \"nw\"\n",
      "  \"pt\"\n",
      "  \"tc\"\n",
      "  \"wb\"\n",
      "]\n",
      "eval_frequency = 5000\n",
      "report_frequency = 100\n",
      "log_root = \"logs\"\n",
      "cluster {\n",
      "  addresses {\n",
      "    ps = [\n",
      "      \"localhost:2222\"\n",
      "    ]\n",
      "    worker = [\n",
      "      \"localhost:2223\"\n",
      "      \"localhost:2224\"\n",
      "    ]\n",
      "  }\n",
      "  gpus = [\n",
      "    0\n",
      "    1\n",
      "  ]\n",
      "}\n",
      "log_dir = \"logs/final\"\n",
      "Loading word embeddings from glove.840B.300d.txt...\n",
      "Done loading word embeddings.\n",
      "Loading word embeddings from glove_50_300_2.txt...\n",
      "Done loading word embeddings.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'predicted_clusters'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Float64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not str",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predicted_clusters'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/scratch/24080438.1.mc_gpu_interactive/ipykernel_144457/3576329932.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcoref_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'e2ecoref'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/sps/humanum/user/sdo/easycoref/src/easycoref/coref.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                     \u001b[0mliste_clusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                     \u001b[0mlist_clusters_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_coref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"predicted_clusters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m                     \u001b[0mlist_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_coref\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                     \u001b[0;31m# Text under list format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sps/humanum/user/sdo/anaconda/envs/Corefenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'predicted_clusters'"
     ]
    }
   ],
   "source": [
    "coref_model.inference(model='e2ecoref')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Visualisation**\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "- **Standardized results**\n",
    "\n",
    "We use df_useful to have the positions of clusters and convert it to \"spans positions\" to have for each column *col* a *span_positions_col*\n",
    "\n",
    "- **Visualisation**\n",
    "\n",
    "Those columns are then used for visualisation to rewrite the text while highlighting mentions of the same coreference chain in the same colour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coref_model.visualisation(model='e2ecoref',col='text1',i=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
